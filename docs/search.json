[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monitoring African Urban Growth using Earth Observation",
    "section": "",
    "text": "Welcome to the Urban Expansion Monitoring practical\nThis practical is part of LISS 2113 (Introduction to Geographic Information System and Spatial Data Analysis.)\nThis practical will show you how to use optical Earth Observation data from the Landsat family of satellites to monitor urban expansion in West African cities. The practical uses Google Earth Engine to access the data and perform most of the tasks.\n\n\n\n\n\n\nTip\n\n\n\nWhile GEE is now free to use for non commercial applications, the techniques are very general, and you can use them in other platforms/environments, and for other mapping/classification tasks.\n\n\n\nLearning Objectives\nBy the end of this practical, you will have had experience in * Access and filter large-scale satellite image collections (Landsat). * Implement urban detection algorithms using JavaScript in GEE. * Quantify spatial growth rates for a chosen city. * Export publication-quality maps and data.\n\n\nPractical Structure\nThe workshop is divided into three main stages:\n\nIntroduction: Some background reading and other useful information.\nData & methods: A detailed description of the different tasks that you’ll undertake.\nBuild annual Landsat composite images: Use GEE to build annual base images that hopefully easily reflect different land cover types.\nTrain, validate and apply a “Random Forest” classifier: Now, we’ll use a machine learning method called “Random Forest” to build a classifier, validate that it works, and apply to our composite images\nTracking urban growth. Discussion: Use your work to address a research question!\n\n\n\n\n\n\n\nImportantPrerequisites\n\n\n\nThis practical assumes you have a Google Earth Engine account approved. If you do not, please notify the instructor immediately as approval can take a few days.\n\n\n\n\nFurther reading\nThis practical shows a typical application of EO data for research, but doesn’t cover fundamental EO concepts, basics of statistics/Machine Learning, or even how to use Google Earth Engine. For that you’ll need additional sources of information:\n\nNatural Resources Canada Basics of Remote Sensing\nNASA Applied Scinece Fundamentals of Remote Sensing ARSET\nLund University Introduction to Remote Sensing and Geographical Information Systems\nRandom forest in remote sensing: A review of applications and future directions Belgiu and Dragut (2016)\nGetting started with Google Earth Engine\nGEE community tutorials\nGEE Javascript API\n\n\n\nGetting Help\nIf you get stuck during the code blocks:\n\nUse Search Engines!!\nCheck the Console for specific error messages.\nUse the print() function to inspect variables.\nReference the Official GEE Documentation.\n\nIf you are happy using Large Language Models (LLMs), you can get a lot of very uesful help from them: * You can ask the LLM to “Explain this code to me in really simple terms” * Modify the code to e.g. use Python rather than Javascript if you see it fit * Explore additional datasets"
  },
  {
    "objectID": "full-manual.html",
    "href": "full-manual.html",
    "title": "Discussion",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\n\n\n\n\n\nflowchart LR\n    %% Define the nodes with simplified text\n    A[\"**1. Data Acquisition**&lt;br/&gt;Landsat & Sentinel\"]\n    B[\"**2. Pre-processing**&lt;br/&gt;Filtering & Mosaicking\"]\n    C[\"**3. Classification**&lt;br/&gt;Random Forest ML\"]\n    D[\"**4. Analysis**&lt;br/&gt;Urban Growth Stats\"]\n\n    %% Simple linear connections\n    A --&gt; B --&gt; C --&gt; D\n\n    %% Clean, professional styling\n    style A fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px\n    style B fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px\n    style C fill:#E3F2FD,stroke:#1976D2,stroke-width:2px\n    style D fill:#FFEBEE,stroke:#C62828,stroke-width:2px\n\n    %% Global font settings for the diagram\n    linkStyle default stroke:#666,stroke-width:2px;\n\n\n Overall flow chart \n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth."
  },
  {
    "objectID": "full-manual.html#introduction",
    "href": "full-manual.html#introduction",
    "title": "Discussion",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\n\n\n\n\n\nflowchart LR\n    %% Define the nodes with simplified text\n    A[\"**1. Data Acquisition**&lt;br/&gt;Landsat & Sentinel\"]\n    B[\"**2. Pre-processing**&lt;br/&gt;Filtering & Mosaicking\"]\n    C[\"**3. Classification**&lt;br/&gt;Random Forest ML\"]\n    D[\"**4. Analysis**&lt;br/&gt;Urban Growth Stats\"]\n\n    %% Simple linear connections\n    A --&gt; B --&gt; C --&gt; D\n\n    %% Clean, professional styling\n    style A fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px\n    style B fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px\n    style C fill:#E3F2FD,stroke:#1976D2,stroke-width:2px\n    style D fill:#FFEBEE,stroke:#C62828,stroke-width:2px\n\n    %% Global font settings for the diagram\n    linkStyle default stroke:#666,stroke-width:2px;\n\n\n Overall flow chart \n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth."
  },
  {
    "objectID": "full-manual.html#data-and-methods",
    "href": "full-manual.html#data-and-methods",
    "title": "Discussion",
    "section": "Data and methods",
    "text": "Data and methods\n\nGoogle Earth Engine (GEE)\nEven though the Landsat archive is free of charge and there are many ways of accessing it and using the data, here we will use Google Earth Engine.\n\n\n\n\n\n\nImportantGetting a GEE account\n\n\n\nYou will need a Google (or a “gmail”) account and then you can signup for GEE here. Follow the instructions therein\nNote that this process can take a few days to complete!!\n\n\nGEE is a cloud based solution, in which most of the heavy processing is done in Google’s servers. The processing is defined using a high level computing language (we’ll use javascript via the browser here, but Python is another possibility). Generally speaking, we will do most of the processing on GEE via the browser, but will export results (as e.g., raster files) that can then be further explored in other software tools. Figure 1 shows a labelled screenshot of GEE’s main editor window.\n\n\n\n\n\n\nFigure 1: The GEE code editor (from here)\n\n\n\n\n\nAnnual composite creation method\n\nSatellite data\nWe use Landsat surface reflectance data provided through Google Earth Engine. Landsat offers a unique, continuous global record of Earth observations from the 1980s to the present, making it particularly suitable for long-term urban studies. To ensure consistency across time, we combine data from multiple Landsat sensors:\n\nLandsat 5 TM (for earlier years)\nLandsat 7 ETM+\nLandsat 8 OLI\nLandsat 9 OLI-2\n\nAll datasets are taken from the Collection 2, Tier 1, Level-2 Surface Reflectance products. These data have already been radiometrically calibrated and atmospherically corrected, allowing surface reflectance values to be compared across sensors and years.\n\n\nTemporal selection\nThe analysis will be performed for the following years: 2000, 2005, 2010, 2015, 2020, and 2024 These snapshots provide a simple but effective way to visualise long-term urban expansion.\n\n\n\n\n\n\nImportant\n\n\n\nFigure 2 is the typical cloudiness in Bamako (from here)\n\n\n\n\n\n\nFigure 2: Bamako cloudiness/precipitation meteogram\n\n\n\nDoes that suggest anything useful in terms of creating an annual mosaic? Can yuou think of any period(s) that might be most useful and why?\n\n\n\n\nCloud masking and quality control\nSatellite images are affected by clouds, cloud shadows, and other artefacts that can obscure the surface. To address this, we apply a quality mask based on the Landsat QA_PIXEL band. Pixels flagged as any of the following are removed:\n\nFill (missing data)\nDilated cloud\nCirrus cloud\nCloud\nCloud shadow\n\nIn addition, scenes are filtered using metadata to retain only images with less than 70% reported cloud cover. To keep processing efficient, no more than 25 images per year are used, prioritising the clearest scenes.\n\n\nBand harmonisation and scaling\nDifferent Landsat sensors store spectral bands using different band numbers. To allow images from different missions to be combined:\nSpectral bands are renamed to a common set of names (blue, green, red, nir, swir1, swir2). Table 1 shows what these bands represent.\n\n\n\nTable 1: Common Landsat spectral bands\n\n\n\n\n\n\n\n\n\n\n\nBand name\nSpectral region\nApprox. wavelength (nm)\nWhat it is commonly used for\n\n\n\n\nblue\nBlue\n450–510\nWater, haze, urban features\n\n\ngreen\nGreen\n520–600\nVegetation reflectance, urban areas\n\n\nred\nRed\n630–690\nVegetation vs built-up contrast\n\n\nnir\nNear-infrared (NIR)\n760–900\nVegetation health, biomass\n\n\nswir1\nShortwave IR (SWIR)\n1550–1750\nBuilt-up areas, soil, moisture\n\n\nswir2\nShortwave IR (SWIR)\n2080–2350\nUrban materials, dryness, fire scars\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis harmonisation step ensures that reflectance values are directly comparable across sensors and years.\n\n\nSurface reflectance values are rescaled using the official Landsat Collection 2 scaling factors\n\n\nAdditional spectral features (“indices”)\n\n\n\n\n\n\nFigure 3: Sample spectral reflectance of vegetation, crops, built-up, bare soils and water classes (from DOI:10.1016/j.isprsjprs.2017.11.006\n\n\n\nSome simple manipulations of reflectance are useful to differentiate different land cover classes. For example, the large reflectance contrast between the nir and red bands in healthy vegetation (see Figure 3) can be used to detect its prevalence. Similar empirical relationships have been investigated for other scene types, such as urban areas or water. In Table 2 there’s brief summary of these\n\n\n\nTable 2: Useful empirical band combinations (indices)\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nFull name\nFormula (Landsat bands)\nWhat it highlights\nNotes\n\n\n\n\nNDBI\nNormalised Difference Built-up Index\n(swir1 − nir) / (swir1 + nir)\nBuilt-up / impervious surfaces\nSimple, widely used; can confuse bare soil\n\n\nUI\nUrban Index\n(swir2 − nir) / (swir2 + nir)\nDense urban areas\nStrong urban contrast; more noise-sensitive\n\n\nIBI\nIndex-based Built-up Index\n(NDBI − (NDVI + MNDWI)) / (NDBI + (NDVI + MNDWI))\nBuilt-up areas with vegetation and water suppressed\nMore complex but very effective\n\n\nEBBI\nEnhanced Built-up and Bareness Index\n(swir1 − nir) / (10 × √(swir1 + TIR))\nBuilt-up vs bare soil\nRequires thermal band; advanced\n\n\nNDVI\nNormalised Difference Vegetation Index\n(nir − red) / (nir + red)\nVegetation\nOften used as a vegetation mask, not an urban index\n\n\nMNDWI\nModified Normalised Difference Water Index\n(green − swir1) / (green + swir1)\nWater bodies\nUsed within IBI to suppress water\n\n\n\n\n\n\n\n\nAnnual image composites\nFor each year, all selected and processed images are combined into a single annual composite using the median value of each pixel. Using the median:\n\nReduces the influence of residual clouds or outliers\nProduces a more stable and representative image of surface conditions\nIs well suited for urban studies where abrupt extremes are not desired\n\nEach annual composite is clipped to the AOI and assigned a timestamp corresponding to the study year.\n\n\nVisualisation\nFor exploratory analysis, annual composites are displayed as true-colour (RGB) images using the red, green, and blue bands. A consistent reflectance stretch is applied across all years to ensure that visual differences primarily reflect real land surface change rather than display settings.\n\n\nSummary\nIn this first stage of the practical, we:\n1 .Define an urban area of interest 2. Select multi-decadal Landsat surface reflectance data 3. Apply cloud masking and quality filtering 4. Harmonise spectral bands across sensors 5. Create annual composites with different feature sets\nThese annual mosaics form the foundation for later steps in the practical, where we will quantify and interpret urban growth patterns.\n\n\n\nSupervised classification and class definition\nTo map urban expansion, we use a supervised classification approach. In supervised classification, the algorithm “learns” how different land cover types appear in satellite data based on training samples provided by the user. In this practical, you can define three or four land cover classes (e.g., built-up areas, vegetation, bare soil, and water. Although all classes are included to improve discrimination, the primary focus of the analysis is on identifying built-up (urban) areas accurately.\n\n\n\n\n\n\nTip\n\n\n\nHaving additional classes can be useful if there are some uninteresting” classes are very distinct (e.g. water or vegetation) that can confuse the algorithm.\n\n\nTraining data are provided as a labelled feature collection, where each feature represents a location with a known land cover class. These training labels are typically created by manually digitising polygons or points using high-resolution imagery as reference. Each training feature contains a class identifier (e.g. an integer code) that links the sample to one of the four land cover types. Spectral values and indices (e.g. NDBI, NDVI) are extracted from the Landsat composites at these locations and used as input to the classifier.\nWe use a Random Forest (RF) classifier, which is an ensemble machine-learning method based on many decision trees. RF is well suited to remote sensing applications because it handles non-linear relationships, is relatively robust to noise, and performs well with a limited number of training samples. The labelled data are split into training and testing subsets, typically using a random partition (for example, 70% for training and 30% for testing). The training subset is used to build the model, while the testing subset provides an independent estimate of classification accuracy.\nModel performance can be roughly validated using a confusion matrix and overall accuracy derived from the testing data. Additional qualitative validation is encouraged by visually comparing the classified map with true-colour imagery and checking whether built-up areas correspond to known urban features such as dense neighbourhoods and road networks. This combination of quantitative and visual checks will helpyou assess the suitability of your results and whether they are fit for purpose."
  },
  {
    "objectID": "full-manual.html#creating-an-annual-composite-in-gee",
    "href": "full-manual.html#creating-an-annual-composite-in-gee",
    "title": "Discussion",
    "section": "Creating an annual composite in GEE",
    "text": "Creating an annual composite in GEE\nThis is the actual code that performs this. Most of the code is standard boilerplate, so it’s fundamentally a “cut and paste” job, but you should change options and see the results! While all these different snippets are given individually, you should copy them in sequence in the code editor.\nI will use using Bamako for this example, but you’ll have to change this to other cities!\n\nSetting up some variables\nIt’s a good idea to define options and variables once, and keep referring to them. If you want to change things later on, you can do this in a managed way.\n/**************************************************************\n * SETTINGS (you can edit these)\n **************************************************************/\n\n// City and AOI (example: Bamako)\nvar cityName = 'Bamako';\nvar cityPoint = ee.Geometry.Point([-8.0029, 12.6392]);\n\n// AOI size: buffer radius in metres \nvar aoiRadiusMeters = 20000;\n\n// Target years for the practical\nvar targetYears = [2000, 2005, 2010, 2015, 2020, 2024];\n\n// Choose an annual period to calculate the mosaic\nvar seasonStartMonth = 1;  // January\nvar seasonEndMonth   = 4;  // April (end is handled as May 1 below)\n\n// Scene filtering knobs (speed + robustness)\nvar maxCloudCoverPercent = 70; // pre-filter scenes by metadata cloud cover\nvar maxScenesPerYear = 25;     // keep only the clearest N scenes (big speed win)\n\n// Visualisation stretch for reflectance RGB/false colour (scaled reflectance ~0–1)\nvar rgbVis = {min: 0.0, max: 0.30};\n\n\nMap setup\nWe will also select our region (simply my creating a circular buffer around the centre lat/lon of Bamako)\n/**************************************************************\n * MAP SETUP (AOI, quick sanity check)\n **************************************************************/\n\nvar aoi = cityPoint.buffer(aoiRadiusMeters); // simple circular AOI (robust geometry)\n\nMap.centerObject(aoi, 12); // 12 is the initial zoom level\nMap.addLayer(aoi, {color: 'yellow'}, cityName + ' AOI');\n\n// AOI area sanity check (needs a small maxError)\nprint('AOI area (km^2):', aoi.area(1).divide(1e6));\n\n\nSelecting Landsat data\nThe following snippet selects the different Landsat collections (we need different sensors to cover our large temporal period). The Landsat L2 collection has a per pixel field callec QA_PIXEL that encodes whether a cloud or shadow has been detected. It also uses a dilated mask to try to mask cloud shadows. We then need to scale the data to units of reflectance (a number between 0 and 1), and additionally, we remove (mask) any pixels where the reflectance is outside this range (can happen due to to processing issues), or whether any of the bands are missing.\n/**************************************************************\n * LOAD RAW LANDSAT COLLECTIONS (Collection 2, Level-2)\n *\n * We keep these “raw” collections separate from preprocessing.\n * Important idea: filter on metadata first, then map() preprocessing.\n **************************************************************/\n\nvar landsat5_raw = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'); // Landsat 5 TM\nvar landsat7_raw = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'); // Landsat 7 ETM+\nvar landsat8_raw = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2'); // Landsat 8 OLI\nvar landsat9_raw = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'); // Landsat 9 OLI-2\n\n\n/**************************************************************\n * CLOUD/SHADOW MASKING (using QA_PIXEL)\n *\n * We use QA_PIXEL bits to remove:\n *  - Fill\n *  - Dilated cloud\n *  - Cirrus\n *  - Cloud\n *  - Cloud shadow\n *\n * This is deliberately “simple but decent” for a taster practical.\n **************************************************************/\n\nfunction qaCloudShadowMask_C2L2(image) {\n  var qa = image.select('QA_PIXEL');\n\n  // Build a single-band boolean mask:\n  // 1 means “keep”, 0 means “mask out”\n  var mask = qa.bitwiseAnd(1 &lt;&lt; 0).eq(0)   // not fill\n    .and(qa.bitwiseAnd(1 &lt;&lt; 1).eq(0))      // not dilated cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 2).eq(0))      // not cirrus\n    .and(qa.bitwiseAnd(1 &lt;&lt; 3).eq(0))      // not cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 4).eq(0));     // not cloud shadow\n\n  return mask;\n}\n\n\n/**************************************************************\n * REFLECTANCE SCALING + \"FEATURE HYGIENE\"\n *\n * Landsat C2 L2 SR scaling:\n *   SR = DN * 0.0000275 + (-0.2)\n *\n * \n *  - Clamp reflectance to [0, 1] to avoid odd outliers.\n *  - Mask pixels where any band is missing (edges, artefacts).\n **************************************************************/\n\nfunction scaleAndCleanReflectance(srImage) {\n  // Apply the scaling\n  var scaled = srImage.multiply(0.0000275).add(-0.2);\n\n  // Clamp to a sensible range\n  scaled = scaled.clamp(0, 1);\n\n  // Mask out pixels where any band is missing/invalid\n  // (reduce(min) checks for missing values across bands)\n  var allBandsPresent = scaled.reduce(ee.Reducer.min()).gte(0);\n  return scaled.updateMask(allBandsPresent);\n}\n\n\nHarmonise sensors\nThe different Landsat sensors have different band names. Here, we relabel them to a common set of names. We use the previous code snippets to apply the cloud mask, reflectance scaling and additional cleaning up to the data.\n/**************************************************************\n * SENSOR HARMONISATION (common band names)\n *\n * Landsat 5/7 SR bands: SR_B1..SR_B5, SR_B7 (Band 6 is thermal ST_B6)\n * Landsat 8/9 SR bands: SR_B2..SR_B7 (Band 1 is coastal; we ignore it)\n *\n * We rename to a common 6-band set:\n *   blue, green, red, nir, swir1, swir2\n **************************************************************/\n\nfunction prepLandsat57(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B1','SR_B2','SR_B3','SR_B4','SR_B5','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\nfunction prepLandsat89(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\n\nAdd indices\nIn many applications, spectral band combinations can be useful to enhance the signal and facilitate classification. They’re also quite useful to visually interpret images qualitatively, so we’ll add a bunch of them here.\n/**************************************************************\n * ADD INDICES (extra feature bands for classification)\n *\n * Indices included:\n *  - NDVI  (vegetation)\n *  - MNDWI (water)\n *  - NDBI  (built-up proxy)\n *  - UI    (simple urban index): UI = NDBI - NDVI (easy to explain)\n *  - IBI   (index-based built-up index): contrasts built-up vs veg+water\n *  - BRIGHT (optional): mean reflectance as a simple brightness feature\n *\n * Note: There are multiple definitions in the literature;\n **************************************************************/\n\nfunction safeDivide(numerator, denominator) {\n  // Avoid divide-by-zero (if den == 0, set it to 1)\n  denominator = denominator.where(denominator.eq(0), 1);\n  return numerator.divide(denominator);\n}\n\nfunction addFeatureIndices(img) {\n  // These assume our common bands exist: red, green, nir, swir1\n  var ndvi  = img.normalizedDifference(['nir', 'red']).rename('NDVI');\n  var mndwi = img.normalizedDifference(['green', 'swir1']).rename('MNDWI');\n  var ndbi  = img.normalizedDifference(['swir1', 'nir']).rename('NDBI');\n\n  // Simple urban index (teachable): built-up proxy minus vegetation\n  var ui = ndbi.subtract(ndvi).rename('UI');\n\n  // IBI (common formulation)\n  var avgVegWater = ndvi.add(mndwi).divide(2);\n  var ibi = safeDivide(ndbi.subtract(avgVegWater), ndbi.add(avgVegWater)).rename('IBI');\n\n  // Brightness: average reflectance (very intuitive for students)\n  var bright = img.select(['blue','green','red','nir','swir1','swir2'])\n    .reduce(ee.Reducer.mean())\n    .rename('BRIGHT');\n\n  return img.addBands([ndvi, mndwi, ndbi, ui, ibi, bright]);\n}\n\n\nBuild a per year image collection\nNow, all the stuff we defined above will start to get used. We need to build a stack of cleaned images per year that cover our region of interest, are masked for clouds etc. For years before 2013, we’ll use Landsat 5 and 7, for years afterwards, we’ll use Landsat 8 and 9. Note that we’re using the variables defined at the top to control things like maximum cloud coverage etc.\n/**************************************************************\n * BUILD A PER-YEAR IMAGE COLLECTION (filter first, then map)\n *\n * Steps:\n *  - Choose a sensor group depending on year (&lt;=2012 uses L5+L7; &gt;=2013 uses L8+L9)\n *  - Filter by AOI and dry-season date window\n *  - Filter by CLOUD_COVER metadata, sort clearest-first, keep top N scenes\n *  - Map preprocessing (mask + scale + rename bands)\n **************************************************************/\n\nfunction getSeasonDateRange(year) {\n  year = ee.Number(year);\n\n  var start = ee.Date.fromYMD(year, seasonStartMonth, 1);\n\n  // endMonth is inclusive-ish: we use the first day of the next month as end\n  var end = ee.Date.fromYMD(year, seasonEndMonth + 1, 1);\n\n  return {start: start, end: end};\n}\n\nfunction collectionForYear(year) {\n  year = ee.Number(year);\n  var range = getSeasonDateRange(year);\n\n  // Choose sensors based on era\n  var rawCollection = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      landsat5_raw.merge(landsat7_raw),   // 2000–2012 era\n      landsat8_raw.merge(landsat9_raw)    // 2013+ era\n    )\n  );\n\n  // Filter on raw metadata BEFORE preprocessing\n  var filtered = rawCollection\n    .filterBounds(aoi)\n    .filterDate(range.start, range.end)\n    .filterMetadata('CLOUD_COVER', 'less_than', maxCloudCoverPercent)\n    .sort('CLOUD_COVER')\n    .limit(maxScenesPerYear);\n\n  // Now map preprocessing and band harmonisation\n  var prepped = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      filtered.map(prepLandsat57),\n      filtered.map(prepLandsat89)\n    )\n  );\n\n  return prepped;\n}\n\n\nThe annual composite!\nYou thought we’d never get here! The next bit defines how the composite gets done (using the median), adds the relevant indices and applies it to all the years.\n/**************************************************************\n * ANNUAL COMPOSITE (median) + indices\n *\n * The “annual mosaic” is the median of all (masked) images in the season.\n * Then we add indices so the final image is ready for classification.\n **************************************************************/\n\nfunction annualComposite(year) {\n  year = ee.Number(year);\n\n  var col = collectionForYear(year);\n\n  // Diagnostics: useful for teaching and debugging\n  print('Year', year, 'images used:', col.size());\n\n  // Build the composite and add feature indices\n  var composite = addFeatureIndices(col.median())\n    .clip(aoi)\n    .set('year', year)\n    // time_start used later if we chart time series\n    .set('system:time_start', ee.Date.fromYMD(year, 3, 1).millis());\n\n  return composite;\n}\n\n// Build mosaics for all target years\nvar annualMosaics = ee.ImageCollection(targetYears.map(annualComposite));\nprint('Annual mosaics:', annualMosaics);\n\n\nDisplay the composite\n/**************************************************************\n * DISPLAY (choose one year to show)\n *\n * Show:\n *  - RGB (natural colour)\n *  - False colour (NIR / red / green) helps see vegetation strongly\n *  - Indices (NDVI, MNDWI, NDBI, UI, IBI)\n *\n * Note: we use visualize() for the RGB layers because it makes map\n * rendering snappier in the Code Editor.\n **************************************************************/\n\nvar displayYear = 2020;\nvar mosaicToShow = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', displayYear)).first()\n);\n\n// Always sanity-check the bands (helpful if something returns empty)\nprint('Bands in displayed mosaic:', mosaicToShow.bandNames());\n\n// RGB and false colour previews\nMap.addLayer(\n  mosaicToShow.select(['red','green','blue']).visualize(rgbVis),\n  {},\n  'RGB ' + displayYear\n);\n\nMap.addLayer(\n  mosaicToShow.select(['nir','red','green']).visualize(rgbVis),\n  {},\n  'False colour (NIR/R/G) ' + displayYear\n);\n\n// Index layers (simple min/max for teaching; no palettes needed)\nMap.addLayer(mosaicToShow.select('NDVI'),  {min: -0.2, max: 0.8}, 'NDVI');\nMap.addLayer(mosaicToShow.select('MNDWI'), {min: -0.6, max: 0.6}, 'MNDWI');\nMap.addLayer(mosaicToShow.select('NDBI'),  {min: -0.6, max: 0.6}, 'NDBI');\nMap.addLayer(mosaicToShow.select('UI'),    {min: -1.0, max: 1.0}, 'UI (= NDBI - NDVI)');\nMap.addLayer(mosaicToShow.select('IBI'),   {min: -1.0, max: 1.0}, 'IBI');\nMap.addLayer(mosaicToShow.select('BRIGHT'),{min: 0.0,  max: 0.4}, 'Brightness');\n\n\nExport sample display year GeoTIFF to Google Drive\nFor repeatibility and further analysis, we’ll export the data to your Google Drive. We’ll use the standard GeoTIFF format, with internal compression to make the data more manageable. This snippet exports a single year, make sure you’re happy with it before exporting all the years.\n/**************************************************************\n * EXPORT ONE YEAR MOSAIC TO GOOGLE DRIVE\n *\n * Pattern:\n *  - Export ONE year first (students learn exports without waiting ages)\n *  - Later, export all years or the classification outputs\n **************************************************************/\n\n// Uncomment to export the displayed year mosaic (multiband)\n \nExport.image.toDrive({\n  image: mosaicToShow, // includes SR bands + indices\n  description: cityName + '_Mosaic_' + displayYear,\n  folder: 'GEE_exports',\n  fileNamePrefix: cityName + '_Mosaic_' + displayYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13,\n  fileFormat: 'GeoTIFF',\n  formatOptions: {\n    cloudOptimized: true,   // Makes it a COG\n    compression: 'DEFLATE' // Powerful lossless compression\n  }\n});\n\n\n\n\nExport all the years\nOnce you’re happy with the single year export, it’s time to expoert all the years in one go.\n/**************************************************************\n * EXPORT ALL YEARS (one multiband GeoTIFF per year)\n * - Exports each annual mosaic (SR bands + indices) to Google Drive\n * - GeoTIFF options: cloud-optimised (COG) + DEFLATE compression\n **************************************************************/\n\n// Folder in Google Drive\nvar exportFolder = 'GEE_exports';\n\n// Choose a scale appropriate for Landsat SR\nvar exportScale = 30;\n\n// Loop over years and create one export task per year\ntargetYears.forEach(function(year) {\n\n  var img = ee.Image(\n    annualMosaics.filter(ee.Filter.eq('year', year)).first()\n  );\n\n  Export.image.toDrive({\n    image: img,                       // multiband: SR + indices\n    description: cityName + '_Mosaic_' + year,\n    folder: exportFolder,\n    fileNamePrefix: cityName + '_Mosaic_' + year,\n    region: aoi,\n    scale: exportScale,\n    maxPixels: 1e13,\n    fileFormat: 'GeoTIFF',\n    formatOptions: {\n      cloudOptimized: true,\n      compression: 'DEFLATE'\n    }\n  });\n\n});\n\n\nExporting annual mosaics as Earth Engine Assets\nSo far, we have used Google Earth Engine to build annual mosaics for our city of interest. These mosaics exist only temporarily inside the script unless we explicitly save them. In addition to exporting images to Google Drive, Earth Engine allows us to export data as Assets, which are stored permanently within our Earth Engine account. Saving the mosaics as Assets is useful because they can be imported directly into other scripts without re-running the mosaicking workflow, making it easier to separate data preparation from later analysis steps such as classification or time-series analysis.\nTo export an annual mosaic as an Asset, we select one of the mosaics from our annualMosaics ImageCollection and use Export.image.toAsset(). When doing this, we must specify:\n\nan asset ID, which defines where the image will be stored in our Earth Engine Assets,\nthe region to export (our AOI),\nthe spatial resolution (30 m for Landsat data).\n\nThe example below exports the mosaic for a single year. Once the export task has completed, the image will appear in the Assets tab of the Earth Engine Code Editor and can be loaded in other scripts using its asset path.\n// Select the mosaic for the year we want to export\nvar exportYear = 2020;\n\nvar mosaicToExport = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', exportYear)).first()\n);\n\n// Export the mosaic as a Google Earth Engine Asset\nExport.image.toAsset({\n  image: mosaicToExport,                 // multiband image (SR + indices)\n  description: cityName + '_Mosaic_' + exportYear,\n  assetId: 'users/YOUR_USERNAME/' + cityName + '_Mosaic_' + exportYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13\n}); // Change YOUR_USERNAME with your own GEE username!\n\n\n\n\n\n\nWarning\n\n\n\nAfter starting the export, the task will appear in the Tasks tab of the Code Editor. You must manually click Run to begin the export. When it has finished, the mosaic can be reused in other Earth Engine scripts by loading it from the Assets panel, allowing you to work directly with the prepared annual composites without repeating the earlier processing steps."
  },
  {
    "objectID": "full-manual.html#building-a-machine-learning-ml-classifier",
    "href": "full-manual.html#building-a-machine-learning-ml-classifier",
    "title": "Discussion",
    "section": "Building a machine learning (ML) classifier",
    "text": "Building a machine learning (ML) classifier\nWe now have pre-processed the data. We need to define the classifier, the mathematical function that maps from pixel features (e.g. reflectance, indices, etc) to land cover classes. Since we’re using a supervised method, we need to provide a set of known pairs of land cover vs “features” that can be used to “learn” how to label each pixel when only the features are available. In a real world scenario, we’d need some high quality “ground truth”, either by surveying, or by using e.g,, very high resolution (VHR) remote sensing, aircraft or UAV data. Since this is not available, we’ll simplify things a bit: you’ll have to select some training samples from the images you pre-processed. We’ll also just collect samples for one year, train the classifier on that single year, and assume that the mapping is adequate for all other years.\n\nCollecting Training Data in Google Earth Engine\nWe’ll build a categorical training dataset for supervised land cover classification, emphasising manual point collection within the Earth Engine Code Editor. Training data are essential for supervised classifiers because they provide labelled examples that the algorithm uses to “learn” how spectral information corresponds to real land cover types. In this context, classes such as soil, water, vegetation, and built-up (urban) are represented with integer labels (so water could be assigned label 0, built-up 1 and so on).\n\nSetting up training layers\nFirst, you need to create new Feature Collections in the Earth Engine map interface to hold the labelled points. For each land cover class of interest, you:\n\nClick the drawing tools button in the upper left of the map window to enable point drawing.\n\n\n\nUse the point marker tool to place points on the map that clearly represent a particular class based on your background data (e.g. built-up areas, vegetation, water, bare soil).\nIn the Geometry Imports panel, click the gear icon next to the newly created “geometry” layer to rename it to something class-specific (e.g. built_up, vegetation), set “Import as” to FeatureCollection, and add a numeric property called label with a unique integer for that class.\n\n\n\nWe have now defined where the some of the training samples will be stored. Go back to the point maker tool, and click on “+ Add layer”, and repeat the above procedure for another landcover classes so that each class has its own labelled point layer.\n\nYou’ll notice that the different layers have been added to the top of the code editor window, under Imports. That means that you can access them within GEE.\nImages from your prepared 2020 annual Landsat composite should be added as basemaps while doing this. You can also load high-resolution reference imagery (e.g., from the high-res GEE mosaic or your own assets) and toggle between these basemaps so you choose points where the class is confidently known and visible for that season. Make sure your reference imagery time overlaps with your 2020 composite to avoid labelling changes that occurred at different dates.\n\n\nCollecting training points with the mouse\nWhen you have the proper basemaps visible, you collect the training data by manually clicking on the map:\n\nSelect the point layer you have set up for a class in the Geometry Imports panel.\n\nChoose the point marker tool.\n\nClick in the map window to place a point where you are confident the class is present (e.g., a clear urban rooftop for built-up, water body for water).\n\nToggle between your high-resolution basemap and your 2020 composite to make sure the labelled land cover matches both reference and classification imagery.\n\nIf you misplace a point, it can be moved or deleted with the pan hand tool.\n\nBe sure to collect points throughout the entire AOI and include examples from the edges of class boundaries: want to the classifier to learn the variability within each class.\n\nThere is no fixed number of points needed; this is often iterative: you collect, classify, inspect errors, and add more as needed.\n\nOnce all classes have suitable points collected, visualising them helps check that they are well distributed and sufficiently representative. Ideally, you want an even spread so that each class’s points cover the spatial and spectral variability within the AOI. After this, you merge the separate FeatureCollections into one combined collection and export it (for example to Google Drive or as an Earth Engine Asset) for use in classification workflows such as Random Forest.\n\n\nMerging the training set and exporting\nTo merge all the classes into a single object, you can write the following code:\nvar Bamako_training = Forest.merge(Water)\n                     .merge(Vegetation)\n                     .merge(BuildUp) ;\nYou can then save them to a GEE asset and/or a Google Drive export:\nExport.table.toAsset({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n  assetId: 'BamakoTraining2020'\n}); // Exports to a GEE asset that you can re-import on other scripts!\n\nExport.table.toDrive({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n\n}); // As a safety precaution, it's a good idea to also export to a GeoJSON\n    // file that you can e.g. open & modify in QGIS etc.\n\n\n\nTraining, validating and applying the classifier\nNow, let’s use a new script to train, validate and apply the classifier. We’ll need to import your training set and the 2018 annual mosaic into your workspace.\n// The following code is my training set and mosaic. You should have\n//something similar. Or you can go ahead and use mine! ;-)\n//var trainingFC = ee.FeatureCollection(\"users/xgomezdans/BamakoTraining2018\"),\n//     image = ee.Image(\"users/xgomezdans/Bamako_Mosaic_2018\");\n\n// Let's select all the bands to go into the classifier. \n// You can test using a smaller subset and see how th classifier changes.\nvar bands = [\"blue\", \"green\", \"red\", \"nir\", \n             \"swir1\", \"swir2\", \n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"] ;\nimage = image.select(bands);\n\n// We now extract the reflectance, indices for each of the training set sample points\n\nvar samples = image.sampleRegions({\n  collection: trainingFC,\n  properties: ['class'],\n  scale: 30,\n  tileScale: 4\n});\n\n// Add random number for splitting\nvar samplesRandom = samples.randomColumn('random');\n\n// 70% training\nvar trainSet = samplesRandom.filter(ee.Filter.lt('random', 0.7));\n\n// 30% testing\nvar testSet = samplesRandom.filter(ee.Filter.gte('random', 0.7));\n\n// This is the RF algorithm. Takes our trainSet, our bands, and fits it.\n// You may want to change the parameters below, it's a bit of a dark art!\nvar rf = ee.Classifier.smileRandomForest({\n  numberOfTrees: 300,\n  variablesPerSplit: null,\n  minLeafPopulation: 1,\n  bagFraction: 0.7,\n  seed: 42\n}).train({\n  features: trainSet,\n  classProperty: 'class',\n  inputProperties: bands\n});\n\n// Test on  the validation set....\nvar validated = testSet.classify(rf);\n\nvar confusionMatrix = validated.errorMatrix('class', 'classification');\n\nprint('Confusion Matrix', confusionMatrix);\nprint('Overall Accuracy', confusionMatrix.accuracy());\nprint('Kappa', confusionMatrix.kappa());\nprint('Producers Accuracy', confusionMatrix.producersAccuracy());\nprint('Users Accuracy', confusionMatrix.consumersAccuracy());\n\n\n// Now classifiy the 2018 composite using the RF classifier....\n\nvar classified = image.classify(rf);\n\nvar palette = [\n  '#4575b4',  // water\n  '#1a9850', // veg\n  '#d73027', // bare\n  '#fee08b', // builtup\n  \n];\n\nMap.centerObject(trainingFC, 12);\nMap.addLayer(classified,\n  {min: 0, max: 3, palette: palette},\n  'Landcover Classification');\n\n\n// We can also export the classifier to use elsewhere\nExport.classifier.toAsset({\n  classifier: rf,\n  description: 'classifier_export',\n  assetId: \"BamakoRFClassifier\"\n});\n\n// You can load it up again using e.g.\n// var rf = ee.Classifier.load(assetId)\n\nIn my data set, the overall accuracy was around 82%, with a \\(\\kappa\\) value around 0.74, which suggests that on the same year, and around Bamako, the classifier is pretty good. The confusion matrix in Table 3 gives as an interesting vista of the classifier’s performance:\n\n\n\nTable 3: Confusion matrix for Bamako example (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference  Predicted\nWater (0)\nVegetation (1)\nBare (2)\nBuilt-up (3)\nRow total\n\n\n\n\nWater (0)\n47\n0\n1\n0\n48\n\n\nVegetation (1)\n0\n32\n2\n6\n40\n\n\nBare (2)\n1\n13\n23\n13\n50\n\n\nBuilt-up (3)\n0\n3\n7\n114\n124\n\n\nColumn total\n48\n48\n33\n133\n262\n\n\n\n\n\n\nWe can make the following observations:\n\nWater: This is the most accurately classified category, with 47 out of 48 instances correctly identified.\n`Built up: This class shows strong performance with 114 correct predictions (92%), though it occasionally absorbs misclassifications from Bare Soil and Veg.\nThe biggest error occurs in the Bare Soil class, where only 23 out of 50 instances were correctly identified. A large portion of Bare Soil (13 instances) was misclassified as Vegetation, and another 13 were misclassified as Built up. This suggests a spectral similarity between these land cover types that the model is finding difficult to resolve.\n\n\n\n\n\n\n\nNote\n\n\n\nWhat do you think causes this confusions?\n\n\n\n\nInterpreting the classifier\nML methods are often defined as “black box” methos, in the sense that decisions they make are not obvious. We can try to shed some light on this by calculating some metrics and plotting scatterplots. For example, Figure 4 shows the normlised Feature Importance for all the used Features. While MNDWI (the “water” index) is a bit higher than all the others, it looks as if the RF is using all the available information without being biased by any single features. Figure 5 shows the scatterplot between the NDVI and NDBI, but it’s not very informative: clearly, water is in the bottom right hand side, but we can see that all the other classes appear clustered in the top left, with quite a bit of overlap.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think this is happening?\nCan you think of some other strategy to improve separation?\n\n\n\n\n\n\n\n\nFigure 4: Normlised Feature imortance for Bamako classifier\n\n\n\n\n\n\n\n\n\nFigure 5: Scatterplot of NDVI vs NDBI"
  },
  {
    "objectID": "full-manual.html#tracking-urban-growth-using-eo.-discussion",
    "href": "full-manual.html#tracking-urban-growth-using-eo.-discussion",
    "title": "Discussion",
    "section": "Tracking urban growth using EO. Discussion",
    "text": "Tracking urban growth using EO. Discussion\n\nCounting pixels\nOnce you have your individual land cover masks, you can count pixels and plot the time series. The following script can do that for you and plot a couple of interesting masks:\n/**** Assets ****/\nvar classifier = ee.Classifier.load('users/xgomezdans/BamakoRFClassifier');\n\nvar training = ee.FeatureCollection('users/xgomezdans/BamakoTraining2018');\nvar roi = training.geometry().bounds();\n\nvar bands = [\"blue\", \"green\", \"red\", \"nir\",\n             \"swir1\", \"swir2\",\n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"];\n\nvar palette = [\n  '#4575b4', // 0 water\n  '#1a9850', // 1 veg\n  '#d73027', // 2 bare\n  '#fee08b'  // 3 builtup\n];\n\n/**** Client-side list of mosaics (IMPORTANT) ****/\nvar mosaics = [\n  {year: 2000, path: 'users/xgomezdans/Bamako_Mosaic_2000'},\n  {year: 2005, path: 'users/xgomezdans/Bamako_Mosaic_2005'},\n  {year: 2010, path: 'users/xgomezdans/Bamako_Mosaic_2010'},\n  {year: 2015, path: 'users/xgomezdans/Bamako_Mosaic_2015'},\n  {year: 2018, path: 'users/xgomezdans/Bamako_Mosaic_2018'},\n  {year: 2020, path: 'users/xgomezdans/Bamako_Mosaic_2020'},\n  {year: 2024, path: 'users/xgomezdans/Bamako_Mosaic_2024'}\n  // {year: 2025, path: 'users/xgomezdans/Bamako_Mosaic_2025'}\n];\n\n/**** Per-year pixel counts (server-side computations, but constant asset IDs) ****/\nfunction countsFeature(year, path) {\n  var img = ee.Image(path).select(bands);\n  var cls = img.classify(classifier).rename('class');\n\n  var scale = img.projection().nominalScale();\n\n  var hist = ee.Dictionary(\n    cls.reduceRegion({\n      reducer: ee.Reducer.frequencyHistogram(),\n      geometry: roi,\n      scale: scale,\n      bestEffort: true,\n      maxPixels: 1e13\n    }).get('class')\n  );\n\n  return ee.Feature(null, {\n    year: year,\n    class0: ee.Number(hist.get('0', 0)),\n    class1: ee.Number(hist.get('1', 0)),\n    class2: ee.Number(hist.get('2', 0)),\n    class3: ee.Number(hist.get('3', 0))\n  });\n}\n\n/**** Build FeatureCollection ****/\nvar feats = mosaics.map(function(m) {\n  return countsFeature(m.year, m.path);\n});\nvar fc = ee.FeatureCollection(feats).sort('year');\nprint('Pixel counts per class & year', fc);\n\n/**** Plot evolution ****/\nvar chart = ui.Chart.feature.byFeature(fc, 'year', ['class0', 'class1', 'class2', 'class3'])\n  .setChartType('LineChart')\n  .setOptions({\n    title: 'Bamako class pixel counts over time',\n    hAxis: {title: 'Year', format: '####'},\n    vAxis: {title: 'Pixel count'},\n    lineWidth: 2,\n    pointSize: 5,\n    series: {\n      0: {color: palette[0], labelInLegend: '0 water'},\n      1: {color: palette[1], labelInLegend: '1 veg'},\n      2: {color: palette[2], labelInLegend: '2 bare'},\n      3: {color: palette[3], labelInLegend: '3 builtup'}\n    }\n  });\nprint(chart);\n\n/**** Optional: map preview for first & last mosaic ****/\nvar first = mosaics[0];\nvar firstCls = ee.Image(first.path).select(bands).classify(classifier).clip(roi);\n\n\nvar last = mosaics[mosaics.length - 1];\nvar latestCls = ee.Image(last.path).select(bands).classify(classifier).clip(roi);\n\nMap.centerObject(roi, 10);\nMap.addLayer(roi, {}, 'ROI', false);\nMap.addLayer(firstCls, {min: 0, max: 3, palette: palette}, 'Classified ' + first.year);\nMap.addLayer(latestCls, {min: 0, max: 3, palette: palette}, 'Classified ' + last.year);\n\n\nThe Bamako case\n. In Figure 6, I show the evolution of Bamako in Mali. The curve shows a fairly constant growth between 1975 and 2020, growing from around 60 square km to more than 140 square km, but the growth appears to flatten after 2020. Note that this could be an effect of the relatively small area we’re investigating (~20 km) becoming saturated and growth taking place elsewhere outside of our study area.\nWe can also look at the spatial distribution of the growth (see Figure 7). Clearly, Bamako has continue growing along the Niger River, but also outside from it, and we can see how the build up area ends up making a continuum between the main Bamako city and the towns that existed around it in 2000.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in the particular case of Bamako, you can read (Keita et al. (2021)). This paper does something similar to what you have done here.\n\n\n\n\n\n\n\n\nFigure 6: Evolution of the size of build up area for a radial region centred in Bamako (Mali) and extending 20km.\n\n\n\n\n\n\n\n\n\nFigure 7: Built up extent of Bamako in 2000 (red) and 2025 (blue)\n\n\n\n\n\nA typical classification problem\nWhat you’ve got through is a typical classification problem. The goal of the classification (e.g.the “labels” you are interested in) and the input data might be different, but generally speaking, this is the blueprint. There are of course degrees of sophistication you could improve on. For example, in some tasks, the dynamics of the change are what is important. For example, if you are trying to map different types of trees, it might be useful to consider features in summer and winter, to easily separate evergreen and deciduous. Another refinement (specially if your data has a very fine spatial resolution) is to exploit the spatial context: in this practical, we have just taken pixels on their own, but pixels are often spatially correlated, so taking each pixel considering its neighbouring spatial context is a great idea. This is part of what makes so-called deep learning approaches so effective.\nAs EO measurements are quite indirect to what you’re generally after, it pays to spend time removing additional sources of variance in the data, and pre-processing the data to emphasise the characteristics that are useful to your problem. Of course, properly phrasing your problem is also required (do you need to retrieve 15 landcover classes, or are you after one or two?). So simplifying the problem and selecting input features that are strongly linked to you phenomenon of study is generally a good idea.\nThe indirect nature of the problem, augmented by the black box nature of most ML/classification algorithms is a risky prospect: plausible looking results are easy to get, but you need to build certainty on them. This is the role of the validation step: use an independent data set to test the classifier and assess its performance. Do not just consider one single metric, but report a broad suite of metrics, and use them to understand what is confusing the classifier. This exercise can guide you in refining the features you use. For example, in our study case, bare soils, vegetation and built up were confused to different degrees. Since we chose to use data from the dry season, we might be looking at vegetation that is very sparse, and mostly looks like soil. Were this an issue, adding perhaps a second wet season composite might help.\nCollecting training data is fundamental, as it allows you to actually select samples of your requirements and match them up with the EO data. You should take a lot of care in selecting it, and you should err in the side of caution: if unsure, collect more training data. You can always use it to validate things if you don’t use it for training. Ensure that your training set explores the true variability of the classes, and be aware that neighbouring pixels are strongly correlated, so prefer picking single points rather than large polygons. Stratify your sampling, and ensure you collect samples all over the region of interest (you don’t want to biased to a small region). If you’re doing multitemporal analyses like this one, you probably want to collect data at different periods, and covering different sensor configurations.\n\n\nThe most important lesson: Watch out for the EO hubris!!\nPerhaps the most important lesson when thinking about using EO with your projects is to start thinking: “Surely, someone must have had a go at this before me?”. A large number of times, you will find that this is indeed the case, and that there is a large community of researchers that spend years finessing the best way of addressing the problem you’re trying to solve by creating “products”.\nProducts are often freely available, well documented and validated, and if widely used, will have their inadequacies or other “glitches” exposed. For the case at hand, we could have used the Global Human Settlement Layer (described in some detail in Pesaresi et al (2024)). This dataset extends EO data with detailed census data, and has been widely validated. Even if your final task is to use e.g., very high resolution optical data to map slums, it is well worth considering using established products to at least cross-compare or to add additional information that simplifies your task."
  },
  {
    "objectID": "practical.html",
    "href": "practical.html",
    "title": "Monitoring the growth of African cities using EO",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth."
  },
  {
    "objectID": "practical.html#introduction",
    "href": "practical.html#introduction",
    "title": "Monitoring the growth of African cities using EO",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth."
  },
  {
    "objectID": "practical.html#data-and-methods",
    "href": "practical.html#data-and-methods",
    "title": "Monitoring the growth of African cities using EO",
    "section": "Data and methods",
    "text": "Data and methods\n\nGoogle Earth Engine (GEE)\nEven though the Landsat archive is free of charge and there are many ways of accessing it and using the data, here we will use Google Earth Engine.\n\n\n\n\n\n\nImportantGetting a GEE account\n\n\n\nYou will need a Google (or a “gmail”) account and then you can signup for GEE here. Follow the instructions therein\nNote that this process can take a few days to complete!!\n\n\nGEE is a cloud based solution, in which most of the heavy processing is done in Google’s servers. The processing is defined using a high level computing language (we’ll use javascript via the browser here, but Python is another possibility). Generally speaking, we will do most of the processing on GEE via the browser, but will export results (as e.g., raster files) that can then be further explored in other software tools. Figure 1 shows a labelled screenshot of GEE’s main editor window.\n\n\n\n\n\n\nFigure 1: The GEE code editor (from here)"
  },
  {
    "objectID": "practical.html#annual-composite-creation",
    "href": "practical.html#annual-composite-creation",
    "title": "Monitoring the growth of African cities using EO",
    "section": "Annual composite creation",
    "text": "Annual composite creation\n\nSatellite data\nWe use Landsat surface reflectance data provided through Google Earth Engine. Landsat offers a unique, continuous global record of Earth observations from the 1980s to the present, making it particularly suitable for long-term urban studies. To ensure consistency across time, we combine data from multiple Landsat sensors:\n\nLandsat 5 TM (for earlier years)\nLandsat 7 ETM+\nLandsat 8 OLI\nLandsat 9 OLI-2\n\nAll datasets are taken from the Collection 2, Tier 1, Level-2 Surface Reflectance products. These data have already been radiometrically calibrated and atmospherically corrected, allowing surface reflectance values to be compared across sensors and years.\n\n\nTemporal selection\nThe analysis will be performed for the following years: 2000, 2005, 2010, 2015, 2020, and 2024 These snapshots provide a simple but effective way to visualise long-term urban expansion.\n\n\n\n\n\n\nImportant\n\n\n\nFigure 2 is the typical cloudiness in Bamako (from here)\n\n\n\n\n\n\nFigure 2: Bamako cloudiness/precipitation meteogram\n\n\n\nDoes that suggest anything useful in terms of creating an annual mosaic? Can yuou think of any period(s) that might be most useful and why?\n\n\n\n\nCloud masking and quality control\nSatellite images are affected by clouds, cloud shadows, and other artefacts that can obscure the surface. To address this, we apply a quality mask based on the Landsat QA_PIXEL band. Pixels flagged as any of the following are removed:\n\nFill (missing data)\nDilated cloud\nCirrus cloud\nCloud\nCloud shadow\n\nIn addition, scenes are filtered using metadata to retain only images with less than 70% reported cloud cover. To keep processing efficient, no more than 25 images per year are used, prioritising the clearest scenes.\n\n\nBand harmonisation and scaling\nDifferent Landsat sensors store spectral bands using different band numbers. To allow images from different missions to be combined:\nSpectral bands are renamed to a common set of names (blue, green, red, nir, swir1, swir2). Table 1 shows what these bands represent.\n\n\n\nTable 1: Common Landsat spectral bands\n\n\n\n\n\n\n\n\n\n\n\nBand name\nSpectral region\nApprox. wavelength (nm)\nWhat it is commonly used for\n\n\n\n\nblue\nBlue\n450–510\nWater, haze, urban features\n\n\ngreen\nGreen\n520–600\nVegetation reflectance, urban areas\n\n\nred\nRed\n630–690\nVegetation vs built-up contrast\n\n\nnir\nNear-infrared (NIR)\n760–900\nVegetation health, biomass\n\n\nswir1\nShortwave IR (SWIR)\n1550–1750\nBuilt-up areas, soil, moisture\n\n\nswir2\nShortwave IR (SWIR)\n2080–2350\nUrban materials, dryness, fire scars\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis harmonisation step ensures that reflectance values are directly comparable across sensors and years.\n\n\nSurface reflectance values are rescaled using the official Landsat Collection 2 scaling factors\n\n\nAdditional spectral features (“indices”)\n\n\n\n\n\n\nFigure 3: Sample spectral reflectance of vegetation, crops, built-up, bare soils and water classes (from DOI:10.1016/j.isprsjprs.2017.11.006\n\n\n\nSome simple manipulations of reflectance are useful to differentiate different land cover classes. For example, the large reflectance contrast between the nir and red bands in healthy vegetation (see Figure 3) can be used to detect its prevalence. Similar empirical relationships have been investigated for other scene types, such as urban areas or water. In Table 2 there’s brief summary of these\n\n\n\nTable 2: Useful empirical band combinations (indices)\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nFull name\nFormula (Landsat bands)\nWhat it highlights\nNotes\n\n\n\n\nNDBI\nNormalised Difference Built-up Index\n(swir1 − nir) / (swir1 + nir)\nBuilt-up / impervious surfaces\nSimple, widely used; can confuse bare soil\n\n\nUI\nUrban Index\n(swir2 − nir) / (swir2 + nir)\nDense urban areas\nStrong urban contrast; more noise-sensitive\n\n\nIBI\nIndex-based Built-up Index\n(NDBI − (NDVI + MNDWI)) / (NDBI + (NDVI + MNDWI))\nBuilt-up areas with vegetation and water suppressed\nMore complex but very effective\n\n\nEBBI\nEnhanced Built-up and Bareness Index\n(swir1 − nir) / (10 × √(swir1 + TIR))\nBuilt-up vs bare soil\nRequires thermal band; advanced\n\n\nNDVI\nNormalised Difference Vegetation Index\n(nir − red) / (nir + red)\nVegetation\nOften used as a vegetation mask, not an urban index\n\n\nMNDWI\nModified Normalised Difference Water Index\n(green − swir1) / (green + swir1)\nWater bodies\nUsed within IBI to suppress water\n\n\n\n\n\n\n\n\nAnnual image composites\nFor each year, all selected and processed images are combined into a single annual composite using the median value of each pixel. Using the median:\n\nReduces the influence of residual clouds or outliers\nProduces a more stable and representative image of surface conditions\nIs well suited for urban studies where abrupt extremes are not desired\n\nEach annual composite is clipped to the AOI and assigned a timestamp corresponding to the study year.\n\n\nVisualisation\nFor exploratory analysis, annual composites are displayed as true-colour (RGB) images using the red, green, and blue bands. A consistent reflectance stretch is applied across all years to ensure that visual differences primarily reflect real land surface change rather than display settings.\n\n\nSummary\nIn this first stage of the practical, we: 1 .Define an urban area of interest 2. Select multi-decadal Landsat surface reflectance data 3. Apply cloud masking and quality filtering 4. Harmonise spectral bands across sensors 5. Create annual composites with different feature sets\nThese annual mosaics form the foundation for later steps in the practical, where we will quantify and interpret urban growth patterns."
  },
  {
    "objectID": "practical.html#supervised-classification-and-class-definition",
    "href": "practical.html#supervised-classification-and-class-definition",
    "title": "Monitoring the growth of African cities using EO",
    "section": "Supervised classification and class definition",
    "text": "Supervised classification and class definition\nTo map urban expansion, we use a supervised classification approach. In supervised classification, the algorithm “learns” how different land cover types appear in satellite data based on training samples provided by the user. In this practical, you can define three or four land cover classes (e.g., built-up areas, vegetation, bare soil, and water. Although all classes are included to improve discrimination, the primary focus of the analysis is on identifying built-up (urban) areas accurately.\n\n\n\n\n\n\nTip\n\n\n\nHaving additional classes can be useful if there are some uninteresting” classes are very distinct (e.g. water or vegetation) that can confuse the algorithm.\n\n\nTraining data are provided as a labelled feature collection, where each feature represents a location with a known land cover class. These training labels are typically created by manually digitising polygons or points using high-resolution imagery as reference. Each training feature contains a class identifier (e.g. an integer code) that links the sample to one of the four land cover types. Spectral values and indices (e.g. NDBI, NDVI) are extracted from the Landsat composites at these locations and used as input to the classifier.\nWe use a Random Forest (RF) classifier, which is an ensemble machine-learning method based on many decision trees. RF is well suited to remote sensing applications because it handles non-linear relationships, is relatively robust to noise, and performs well with a limited number of training samples. The labelled data are split into training and testing subsets, typically using a random partition (for example, 70% for training and 30% for testing). The training subset is used to build the model, while the testing subset provides an independent estimate of classification accuracy.\nModel performance can be roughly validated using a confusion matrix and overall accuracy derived from the testing data. Additional qualitative validation is encouraged by visually comparing the classified map with true-colour imagery and checking whether built-up areas correspond to known urban features such as dense neighbourhoods and road networks. This combination of quantitative and visual checks will helpyou assess the suitability of your results and whether they are fit for purpose."
  },
  {
    "objectID": "04-classifier.html",
    "href": "04-classifier.html",
    "title": "EO Urban Monitoring",
    "section": "",
    "text": "We now have pre-processed the data. We need to define the classifier, the mathematical function that maps from pixel features (e.g. reflectance, indices, etc) to land cover classes. Since we’re using a supervised method, we need to provide a set of known pairs of land cover vs “features” that can be used to “learn” how to label each pixel when only the features are available. In a real world scenario, we’d need some high quality “ground truth”, either by surveying, or by using e.g,, very high resolution (VHR) remote sensing, aircraft or UAV data. Since this is not available, we’ll simplify things a bit: you’ll have to select some training samples from the images you pre-processed. We’ll also just collect samples for one year, train the classifier on that single year, and assume that the mapping is adequate for all other years.\n\n\nWe’ll build a categorical training dataset for supervised land cover classification, emphasising manual point collection within the Earth Engine Code Editor. Training data are essential for supervised classifiers because they provide labelled examples that the algorithm uses to “learn” how spectral information corresponds to real land cover types. In this context, classes such as soil, water, vegetation, and built-up (urban) are represented with integer labels (so water could be assigned label 0, built-up 1 and so on).\n\n\nFirst, you need to create new Feature Collections in the Earth Engine map interface to hold the labelled points. For each land cover class of interest, you:\n\nClick the drawing tools button in the upper left of the map window to enable point drawing.\n\n\n\nUse the point marker tool to place points on the map that clearly represent a particular class based on your background data (e.g. built-up areas, vegetation, water, bare soil).\nIn the Geometry Imports panel, click the gear icon next to the newly created “geometry” layer to rename it to something class-specific (e.g. built_up, vegetation), set “Import as” to FeatureCollection, and add a numeric property called label with a unique integer for that class.\n\n\n\nWe have now defined where the some of the training samples will be stored. Go back to the point maker tool, and click on “+ Add layer”, and repeat the above procedure for another landcover classes so that each class has its own labelled point layer.\n\nYou’ll notice that the different layers have been added to the top of the code editor window, under Imports. That means that you can access them within GEE.\nImages from your prepared 2020 annual Landsat composite should be added as basemaps while doing this. You can also load high-resolution reference imagery (e.g., from the high-res GEE mosaic or your own assets) and toggle between these basemaps so you choose points where the class is confidently known and visible for that season. Make sure your reference imagery time overlaps with your 2020 composite to avoid labelling changes that occurred at different dates.\n\n\n\nWhen you have the proper basemaps visible, you collect the training data by manually clicking on the map:\n\nSelect the point layer you have set up for a class in the Geometry Imports panel.\n\nChoose the point marker tool.\n\nClick in the map window to place a point where you are confident the class is present (e.g., a clear urban rooftop for built-up, water body for water).\n\nToggle between your high-resolution basemap and your 2020 composite to make sure the labelled land cover matches both reference and classification imagery.\n\nIf you misplace a point, it can be moved or deleted with the pan hand tool.\n\nBe sure to collect points throughout the entire AOI and include examples from the edges of class boundaries: want to the classifier to learn the variability within each class.\n\nThere is no fixed number of points needed; this is often iterative: you collect, classify, inspect errors, and add more as needed.\n\nOnce all classes have suitable points collected, visualising them helps check that they are well distributed and sufficiently representative. Ideally, you want an even spread so that each class’s points cover the spatial and spectral variability within the AOI. After this, you merge the separate FeatureCollections into one combined collection and export it (for example to Google Drive or as an Earth Engine Asset) for use in classification workflows such as Random Forest.\n\n\n\nTo merge all the classes into a single object, you can write the following code:\nvar Bamako_training = Forest.merge(Water)\n                     .merge(Vegetation)\n                     .merge(BuildUp) ;\nYou can then save them to a GEE asset and/or a Google Drive export:\nExport.table.toAsset({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n  assetId: 'BamakoTraining2020'\n}); // Exports to a GEE asset that you can re-import on other scripts!\n\nExport.table.toDrive({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n\n}); // As a safety precaution, it's a good idea to also export to a GeoJSON\n    // file that you can e.g. open & modify in QGIS etc.\n\n\n\n\nNow, let’s use a new script to train, validate and apply the classifier. We’ll need to import your training set and the 2018 annual mosaic into your workspace.\n// The following code is my training set and mosaic. You should have\n//something similar. Or you can go ahead and use mine! ;-)\n//var trainingFC = ee.FeatureCollection(\"users/xgomezdans/BamakoTraining2018\"),\n//     image = ee.Image(\"users/xgomezdans/Bamako_Mosaic_2018\");\n\n// Let's select all the bands to go into the classifier. \n// You can test using a smaller subset and see how th classifier changes.\nvar bands = [\"blue\", \"green\", \"red\", \"nir\", \n             \"swir1\", \"swir2\", \n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"] ;\nimage = image.select(bands);\n\n// We now extract the reflectance, indices for each of the training set sample points\n\nvar samples = image.sampleRegions({\n  collection: trainingFC,\n  properties: ['class'],\n  scale: 30,\n  tileScale: 4\n});\n\n// Add random number for splitting\nvar samplesRandom = samples.randomColumn('random');\n\n// 70% training\nvar trainSet = samplesRandom.filter(ee.Filter.lt('random', 0.7));\n\n// 30% testing\nvar testSet = samplesRandom.filter(ee.Filter.gte('random', 0.7));\n\n// This is the RF algorithm. Takes our trainSet, our bands, and fits it.\n// You may want to change the parameters below, it's a bit of a dark art!\nvar rf = ee.Classifier.smileRandomForest({\n  numberOfTrees: 300,\n  variablesPerSplit: null,\n  minLeafPopulation: 1,\n  bagFraction: 0.7,\n  seed: 42\n}).train({\n  features: trainSet,\n  classProperty: 'class',\n  inputProperties: bands\n});\n\n// Test on  the validation set....\nvar validated = testSet.classify(rf);\n\nvar confusionMatrix = validated.errorMatrix('class', 'classification');\n\nprint('Confusion Matrix', confusionMatrix);\nprint('Overall Accuracy', confusionMatrix.accuracy());\nprint('Kappa', confusionMatrix.kappa());\nprint('Producers Accuracy', confusionMatrix.producersAccuracy());\nprint('Users Accuracy', confusionMatrix.consumersAccuracy());\n\n\n// Now classifiy the 2018 composite using the RF classifier....\n\nvar classified = image.classify(rf);\n\nvar palette = [\n  '#4575b4',  // water\n  '#1a9850', // veg\n  '#d73027', // bare\n  '#fee08b', // builtup\n  \n];\n\nMap.centerObject(trainingFC, 12);\nMap.addLayer(classified,\n  {min: 0, max: 3, palette: palette},\n  'Landcover Classification');\n\n\n// We can also export the classifier to use elsewhere\nExport.classifier.toAsset({\n  classifier: rf,\n  description: 'classifier_export',\n  assetId: \"BamakoRFClassifier\"\n});\n\n// You can load it up again using e.g.\n// var rf = ee.Classifier.load(assetId)\n\nIn my data set, the overall accuracy was around 82%, with a \\(\\kappa\\) value around 0.74, which suggests that on the same year, and around Bamako, the classifier is pretty good. The confusion matrix in Table 1 gives as an interesting vista of the classifier’s performance:\n\n\n\nTable 1: Confusion matrix for Bamako example (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference  Predicted\nWater (0)\nVegetation (1)\nBare (2)\nBuilt-up (3)\nRow total\n\n\n\n\nWater (0)\n47\n0\n1\n0\n48\n\n\nVegetation (1)\n0\n32\n2\n6\n40\n\n\nBare (2)\n1\n13\n23\n13\n50\n\n\nBuilt-up (3)\n0\n3\n7\n114\n124\n\n\nColumn total\n48\n48\n33\n133\n262\n\n\n\n\n\n\nWe can make the following observations:\n\nWater: This is the most accurately classified category, with 47 out of 48 instances correctly identified.\n`Built up: This class shows strong performance with 114 correct predictions (92%), though it occasionally absorbs misclassifications from Bare Soil and Veg.\nThe biggest error occurs in the Bare Soil class, where only 23 out of 50 instances were correctly identified. A large portion of Bare Soil (13 instances) was misclassified as Vegetation, and another 13 were misclassified as Built up. This suggests a spectral similarity between these land cover types that the model is finding difficult to resolve.\n\n\n\n\n\n\n\nNote\n\n\n\nWhat do you think causes this confusions?\n\n\n\n\n\nML methods are often defined as “black box” methos, in the sense that decisions they make are not obvious. We can try to shed some light on this by calculating some metrics and plotting scatterplots. For example, Figure 1 shows the normlised Feature Importance for all the used Features. While MNDWI (the “water” index) is a bit higher than all the others, it looks as if the RF is using all the available information without being biased by any single features. Figure 2 shows the scatterplot between the NDVI and NDBI, but it’s not very informative: clearly, water is in the bottom right hand side, but we can see that all the other classes appear clustered in the top left, with quite a bit of overlap.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think this is happening?\nCan you think of some other strategy to improve separation?\n\n\n\n\n\n\n\n\nFigure 1: Normlised Feature imortance for Bamako classifier\n\n\n\n\n\n\n\n\n\nFigure 2: Scatterplot of NDVI vs NDBI",
    "crumbs": [
      "Download Full PDF",
      "4. Train, validate and run a classifier"
    ]
  },
  {
    "objectID": "04-classifier.html#building-a-machine-learning-ml-classifier",
    "href": "04-classifier.html#building-a-machine-learning-ml-classifier",
    "title": "EO Urban Monitoring",
    "section": "",
    "text": "We now have pre-processed the data. We need to define the classifier, the mathematical function that maps from pixel features (e.g. reflectance, indices, etc) to land cover classes. Since we’re using a supervised method, we need to provide a set of known pairs of land cover vs “features” that can be used to “learn” how to label each pixel when only the features are available. In a real world scenario, we’d need some high quality “ground truth”, either by surveying, or by using e.g,, very high resolution (VHR) remote sensing, aircraft or UAV data. Since this is not available, we’ll simplify things a bit: you’ll have to select some training samples from the images you pre-processed. We’ll also just collect samples for one year, train the classifier on that single year, and assume that the mapping is adequate for all other years.\n\n\nWe’ll build a categorical training dataset for supervised land cover classification, emphasising manual point collection within the Earth Engine Code Editor. Training data are essential for supervised classifiers because they provide labelled examples that the algorithm uses to “learn” how spectral information corresponds to real land cover types. In this context, classes such as soil, water, vegetation, and built-up (urban) are represented with integer labels (so water could be assigned label 0, built-up 1 and so on).\n\n\nFirst, you need to create new Feature Collections in the Earth Engine map interface to hold the labelled points. For each land cover class of interest, you:\n\nClick the drawing tools button in the upper left of the map window to enable point drawing.\n\n\n\nUse the point marker tool to place points on the map that clearly represent a particular class based on your background data (e.g. built-up areas, vegetation, water, bare soil).\nIn the Geometry Imports panel, click the gear icon next to the newly created “geometry” layer to rename it to something class-specific (e.g. built_up, vegetation), set “Import as” to FeatureCollection, and add a numeric property called label with a unique integer for that class.\n\n\n\nWe have now defined where the some of the training samples will be stored. Go back to the point maker tool, and click on “+ Add layer”, and repeat the above procedure for another landcover classes so that each class has its own labelled point layer.\n\nYou’ll notice that the different layers have been added to the top of the code editor window, under Imports. That means that you can access them within GEE.\nImages from your prepared 2020 annual Landsat composite should be added as basemaps while doing this. You can also load high-resolution reference imagery (e.g., from the high-res GEE mosaic or your own assets) and toggle between these basemaps so you choose points where the class is confidently known and visible for that season. Make sure your reference imagery time overlaps with your 2020 composite to avoid labelling changes that occurred at different dates.\n\n\n\nWhen you have the proper basemaps visible, you collect the training data by manually clicking on the map:\n\nSelect the point layer you have set up for a class in the Geometry Imports panel.\n\nChoose the point marker tool.\n\nClick in the map window to place a point where you are confident the class is present (e.g., a clear urban rooftop for built-up, water body for water).\n\nToggle between your high-resolution basemap and your 2020 composite to make sure the labelled land cover matches both reference and classification imagery.\n\nIf you misplace a point, it can be moved or deleted with the pan hand tool.\n\nBe sure to collect points throughout the entire AOI and include examples from the edges of class boundaries: want to the classifier to learn the variability within each class.\n\nThere is no fixed number of points needed; this is often iterative: you collect, classify, inspect errors, and add more as needed.\n\nOnce all classes have suitable points collected, visualising them helps check that they are well distributed and sufficiently representative. Ideally, you want an even spread so that each class’s points cover the spatial and spectral variability within the AOI. After this, you merge the separate FeatureCollections into one combined collection and export it (for example to Google Drive or as an Earth Engine Asset) for use in classification workflows such as Random Forest.\n\n\n\nTo merge all the classes into a single object, you can write the following code:\nvar Bamako_training = Forest.merge(Water)\n                     .merge(Vegetation)\n                     .merge(BuildUp) ;\nYou can then save them to a GEE asset and/or a Google Drive export:\nExport.table.toAsset({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n  assetId: 'BamakoTraining2020'\n}); // Exports to a GEE asset that you can re-import on other scripts!\n\nExport.table.toDrive({\n  collection: Bamako_training,\n  description: 'BamakoTraining2020',\n\n}); // As a safety precaution, it's a good idea to also export to a GeoJSON\n    // file that you can e.g. open & modify in QGIS etc.\n\n\n\n\nNow, let’s use a new script to train, validate and apply the classifier. We’ll need to import your training set and the 2018 annual mosaic into your workspace.\n// The following code is my training set and mosaic. You should have\n//something similar. Or you can go ahead and use mine! ;-)\n//var trainingFC = ee.FeatureCollection(\"users/xgomezdans/BamakoTraining2018\"),\n//     image = ee.Image(\"users/xgomezdans/Bamako_Mosaic_2018\");\n\n// Let's select all the bands to go into the classifier. \n// You can test using a smaller subset and see how th classifier changes.\nvar bands = [\"blue\", \"green\", \"red\", \"nir\", \n             \"swir1\", \"swir2\", \n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"] ;\nimage = image.select(bands);\n\n// We now extract the reflectance, indices for each of the training set sample points\n\nvar samples = image.sampleRegions({\n  collection: trainingFC,\n  properties: ['class'],\n  scale: 30,\n  tileScale: 4\n});\n\n// Add random number for splitting\nvar samplesRandom = samples.randomColumn('random');\n\n// 70% training\nvar trainSet = samplesRandom.filter(ee.Filter.lt('random', 0.7));\n\n// 30% testing\nvar testSet = samplesRandom.filter(ee.Filter.gte('random', 0.7));\n\n// This is the RF algorithm. Takes our trainSet, our bands, and fits it.\n// You may want to change the parameters below, it's a bit of a dark art!\nvar rf = ee.Classifier.smileRandomForest({\n  numberOfTrees: 300,\n  variablesPerSplit: null,\n  minLeafPopulation: 1,\n  bagFraction: 0.7,\n  seed: 42\n}).train({\n  features: trainSet,\n  classProperty: 'class',\n  inputProperties: bands\n});\n\n// Test on  the validation set....\nvar validated = testSet.classify(rf);\n\nvar confusionMatrix = validated.errorMatrix('class', 'classification');\n\nprint('Confusion Matrix', confusionMatrix);\nprint('Overall Accuracy', confusionMatrix.accuracy());\nprint('Kappa', confusionMatrix.kappa());\nprint('Producers Accuracy', confusionMatrix.producersAccuracy());\nprint('Users Accuracy', confusionMatrix.consumersAccuracy());\n\n\n// Now classifiy the 2018 composite using the RF classifier....\n\nvar classified = image.classify(rf);\n\nvar palette = [\n  '#4575b4',  // water\n  '#1a9850', // veg\n  '#d73027', // bare\n  '#fee08b', // builtup\n  \n];\n\nMap.centerObject(trainingFC, 12);\nMap.addLayer(classified,\n  {min: 0, max: 3, palette: palette},\n  'Landcover Classification');\n\n\n// We can also export the classifier to use elsewhere\nExport.classifier.toAsset({\n  classifier: rf,\n  description: 'classifier_export',\n  assetId: \"BamakoRFClassifier\"\n});\n\n// You can load it up again using e.g.\n// var rf = ee.Classifier.load(assetId)\n\nIn my data set, the overall accuracy was around 82%, with a \\(\\kappa\\) value around 0.74, which suggests that on the same year, and around Bamako, the classifier is pretty good. The confusion matrix in Table 1 gives as an interesting vista of the classifier’s performance:\n\n\n\nTable 1: Confusion matrix for Bamako example (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference  Predicted\nWater (0)\nVegetation (1)\nBare (2)\nBuilt-up (3)\nRow total\n\n\n\n\nWater (0)\n47\n0\n1\n0\n48\n\n\nVegetation (1)\n0\n32\n2\n6\n40\n\n\nBare (2)\n1\n13\n23\n13\n50\n\n\nBuilt-up (3)\n0\n3\n7\n114\n124\n\n\nColumn total\n48\n48\n33\n133\n262\n\n\n\n\n\n\nWe can make the following observations:\n\nWater: This is the most accurately classified category, with 47 out of 48 instances correctly identified.\n`Built up: This class shows strong performance with 114 correct predictions (92%), though it occasionally absorbs misclassifications from Bare Soil and Veg.\nThe biggest error occurs in the Bare Soil class, where only 23 out of 50 instances were correctly identified. A large portion of Bare Soil (13 instances) was misclassified as Vegetation, and another 13 were misclassified as Built up. This suggests a spectral similarity between these land cover types that the model is finding difficult to resolve.\n\n\n\n\n\n\n\nNote\n\n\n\nWhat do you think causes this confusions?\n\n\n\n\n\nML methods are often defined as “black box” methos, in the sense that decisions they make are not obvious. We can try to shed some light on this by calculating some metrics and plotting scatterplots. For example, Figure 1 shows the normlised Feature Importance for all the used Features. While MNDWI (the “water” index) is a bit higher than all the others, it looks as if the RF is using all the available information without being biased by any single features. Figure 2 shows the scatterplot between the NDVI and NDBI, but it’s not very informative: clearly, water is in the bottom right hand side, but we can see that all the other classes appear clustered in the top left, with quite a bit of overlap.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think this is happening?\nCan you think of some other strategy to improve separation?\n\n\n\n\n\n\n\n\nFigure 1: Normlised Feature imortance for Bamako classifier\n\n\n\n\n\n\n\n\n\nFigure 2: Scatterplot of NDVI vs NDBI",
    "crumbs": [
      "Download Full PDF",
      "4. Train, validate and run a classifier"
    ]
  },
  {
    "objectID": "05-discussion.html",
    "href": "05-discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "Once you have your individual land cover masks, you can count pixels and plot the time series. The following script can do that for you and plot a couple of interesting masks:\n/**** Assets ****/\nvar classifier = ee.Classifier.load('users/xgomezdans/BamakoRFClassifier');\n\nvar training = ee.FeatureCollection('users/xgomezdans/BamakoTraining2018');\nvar roi = training.geometry().bounds();\n\nvar bands = [\"blue\", \"green\", \"red\", \"nir\",\n             \"swir1\", \"swir2\",\n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"];\n\nvar palette = [\n  '#4575b4', // 0 water\n  '#1a9850', // 1 veg\n  '#d73027', // 2 bare\n  '#fee08b'  // 3 builtup\n];\n\n/**** Client-side list of mosaics (IMPORTANT) ****/\nvar mosaics = [\n  {year: 2000, path: 'users/xgomezdans/Bamako_Mosaic_2000'},\n  {year: 2005, path: 'users/xgomezdans/Bamako_Mosaic_2005'},\n  {year: 2010, path: 'users/xgomezdans/Bamako_Mosaic_2010'},\n  {year: 2015, path: 'users/xgomezdans/Bamako_Mosaic_2015'},\n  {year: 2018, path: 'users/xgomezdans/Bamako_Mosaic_2018'},\n  {year: 2020, path: 'users/xgomezdans/Bamako_Mosaic_2020'},\n  {year: 2024, path: 'users/xgomezdans/Bamako_Mosaic_2024'}\n  // {year: 2025, path: 'users/xgomezdans/Bamako_Mosaic_2025'}\n];\n\n/**** Per-year pixel counts (server-side computations, but constant asset IDs) ****/\nfunction countsFeature(year, path) {\n  var img = ee.Image(path).select(bands);\n  var cls = img.classify(classifier).rename('class');\n\n  var scale = img.projection().nominalScale();\n\n  var hist = ee.Dictionary(\n    cls.reduceRegion({\n      reducer: ee.Reducer.frequencyHistogram(),\n      geometry: roi,\n      scale: scale,\n      bestEffort: true,\n      maxPixels: 1e13\n    }).get('class')\n  );\n\n  return ee.Feature(null, {\n    year: year,\n    class0: ee.Number(hist.get('0', 0)),\n    class1: ee.Number(hist.get('1', 0)),\n    class2: ee.Number(hist.get('2', 0)),\n    class3: ee.Number(hist.get('3', 0))\n  });\n}\n\n/**** Build FeatureCollection ****/\nvar feats = mosaics.map(function(m) {\n  return countsFeature(m.year, m.path);\n});\nvar fc = ee.FeatureCollection(feats).sort('year');\nprint('Pixel counts per class & year', fc);\n\n/**** Plot evolution ****/\nvar chart = ui.Chart.feature.byFeature(fc, 'year', ['class0', 'class1', 'class2', 'class3'])\n  .setChartType('LineChart')\n  .setOptions({\n    title: 'Bamako class pixel counts over time',\n    hAxis: {title: 'Year', format: '####'},\n    vAxis: {title: 'Pixel count'},\n    lineWidth: 2,\n    pointSize: 5,\n    series: {\n      0: {color: palette[0], labelInLegend: '0 water'},\n      1: {color: palette[1], labelInLegend: '1 veg'},\n      2: {color: palette[2], labelInLegend: '2 bare'},\n      3: {color: palette[3], labelInLegend: '3 builtup'}\n    }\n  });\nprint(chart);\n\n/**** Optional: map preview for first & last mosaic ****/\nvar first = mosaics[0];\nvar firstCls = ee.Image(first.path).select(bands).classify(classifier).clip(roi);\n\n\nvar last = mosaics[mosaics.length - 1];\nvar latestCls = ee.Image(last.path).select(bands).classify(classifier).clip(roi);\n\nMap.centerObject(roi, 10);\nMap.addLayer(roi, {}, 'ROI', false);\nMap.addLayer(firstCls, {min: 0, max: 3, palette: palette}, 'Classified ' + first.year);\nMap.addLayer(latestCls, {min: 0, max: 3, palette: palette}, 'Classified ' + last.year);\n\n\n\n. In Figure 1, I show the evolution of Bamako in Mali. The curve shows a fairly constant growth between 1975 and 2020, growing from around 60 square km to more than 140 square km, but the growth appears to flatten after 2020. Note that this could be an effect of the relatively small area we’re investigating (~20 km) becoming saturated and growth taking place elsewhere outside of our study area.\nWe can also look at the spatial distribution of the growth (see Figure 2). Clearly, Bamako has continue growing along the Niger River, but also outside from it, and we can see how the build up area ends up making a continuum between the main Bamako city and the towns that existed around it in 2000.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in the particular case of Bamako, you can read (Keita et al. (2021)). This paper does something similar to what you have done here.\n\n\n\n\n\n\n\n\nFigure 1: Evolution of the size of build up area for a radial region centred in Bamako (Mali) and extending 20km.\n\n\n\n\n\n\n\n\n\nFigure 2: Built up extent of Bamako in 2000 (red) and 2025 (blue)\n\n\n\n\n\n\nWhat you’ve got through is a typical classification problem. The goal of the classification (e.g.the “labels” you are interested in) and the input data might be different, but generally speaking, this is the blueprint. There are of course degrees of sophistication you could improve on. For example, in some tasks, the dynamics of the change are what is important. For example, if you are trying to map different types of trees, it might be useful to consider features in summer and winter, to easily separate evergreen and deciduous. Another refinement (specially if your data has a very fine spatial resolution) is to exploit the spatial context: in this practical, we have just taken pixels on their own, but pixels are often spatially correlated, so taking each pixel considering its neighbouring spatial context is a great idea. This is part of what makes so-called deep learning approaches so effective.\nAs EO measurements are quite indirect to what you’re generally after, it pays to spend time removing additional sources of variance in the data, and pre-processing the data to emphasise the characteristics that are useful to your problem. Of course, properly phrasing your problem is also required (do you need to retrieve 15 landcover classes, or are you after one or two?). So simplifying the problem and selecting input features that are strongly linked to you phenomenon of study is generally a good idea.\nThe indirect nature of the problem, augmented by the black box nature of most ML/classification algorithms is a risky prospect: plausible looking results are easy to get, but you need to build certainty on them. This is the role of the validation step: use an independent data set to test the classifier and assess its performance. Do not just consider one single metric, but report a broad suite of metrics, and use them to understand what is confusing the classifier. This exercise can guide you in refining the features you use. For example, in our study case, bare soils, vegetation and built up were confused to different degrees. Since we chose to use data from the dry season, we might be looking at vegetation that is very sparse, and mostly looks like soil. Were this an issue, adding perhaps a second wet season composite might help.\nCollecting training data is fundamental, as it allows you to actually select samples of your requirements and match them up with the EO data. You should take a lot of care in selecting it, and you should err in the side of caution: if unsure, collect more training data. You can always use it to validate things if you don’t use it for training. Ensure that your training set explores the true variability of the classes, and be aware that neighbouring pixels are strongly correlated, so prefer picking single points rather than large polygons. Stratify your sampling, and ensure you collect samples all over the region of interest (you don’t want to biased to a small region). If you’re doing multitemporal analyses like this one, you probably want to collect data at different periods, and covering different sensor configurations.\n\n\n\nPerhaps the most important lesson when thinking about using EO with your projects is to start thinking: “Surely, someone must have had a go at this before me?”. A large number of times, you will find that this is indeed the case, and that there is a large community of researchers that spend years finessing the best way of addressing the problem you’re trying to solve by creating “products”.\nProducts are often freely available, well documented and validated, and if widely used, will have their inadequacies or other “glitches” exposed. For the case at hand, we could have used the Global Human Settlement Layer (described in some detail in Pesaresi et al (2024)). This dataset extends EO data with detailed census data, and has been widely validated. Even if your final task is to use e.g., very high resolution optical data to map slums, it is well worth considering using established products to at least cross-compare or to add additional information that simplifies your task.",
    "crumbs": [
      "Download Full PDF",
      "5. Discussion"
    ]
  },
  {
    "objectID": "05-discussion.html#tracking-urban-growth-using-eo.-discussion",
    "href": "05-discussion.html#tracking-urban-growth-using-eo.-discussion",
    "title": "Discussion",
    "section": "",
    "text": "Once you have your individual land cover masks, you can count pixels and plot the time series. The following script can do that for you and plot a couple of interesting masks:\n/**** Assets ****/\nvar classifier = ee.Classifier.load('users/xgomezdans/BamakoRFClassifier');\n\nvar training = ee.FeatureCollection('users/xgomezdans/BamakoTraining2018');\nvar roi = training.geometry().bounds();\n\nvar bands = [\"blue\", \"green\", \"red\", \"nir\",\n             \"swir1\", \"swir2\",\n             \"NDVI\", \"MNDWI\", \"NDBI\", \"UI\", \"IBI\", \"BRIGHT\"];\n\nvar palette = [\n  '#4575b4', // 0 water\n  '#1a9850', // 1 veg\n  '#d73027', // 2 bare\n  '#fee08b'  // 3 builtup\n];\n\n/**** Client-side list of mosaics (IMPORTANT) ****/\nvar mosaics = [\n  {year: 2000, path: 'users/xgomezdans/Bamako_Mosaic_2000'},\n  {year: 2005, path: 'users/xgomezdans/Bamako_Mosaic_2005'},\n  {year: 2010, path: 'users/xgomezdans/Bamako_Mosaic_2010'},\n  {year: 2015, path: 'users/xgomezdans/Bamako_Mosaic_2015'},\n  {year: 2018, path: 'users/xgomezdans/Bamako_Mosaic_2018'},\n  {year: 2020, path: 'users/xgomezdans/Bamako_Mosaic_2020'},\n  {year: 2024, path: 'users/xgomezdans/Bamako_Mosaic_2024'}\n  // {year: 2025, path: 'users/xgomezdans/Bamako_Mosaic_2025'}\n];\n\n/**** Per-year pixel counts (server-side computations, but constant asset IDs) ****/\nfunction countsFeature(year, path) {\n  var img = ee.Image(path).select(bands);\n  var cls = img.classify(classifier).rename('class');\n\n  var scale = img.projection().nominalScale();\n\n  var hist = ee.Dictionary(\n    cls.reduceRegion({\n      reducer: ee.Reducer.frequencyHistogram(),\n      geometry: roi,\n      scale: scale,\n      bestEffort: true,\n      maxPixels: 1e13\n    }).get('class')\n  );\n\n  return ee.Feature(null, {\n    year: year,\n    class0: ee.Number(hist.get('0', 0)),\n    class1: ee.Number(hist.get('1', 0)),\n    class2: ee.Number(hist.get('2', 0)),\n    class3: ee.Number(hist.get('3', 0))\n  });\n}\n\n/**** Build FeatureCollection ****/\nvar feats = mosaics.map(function(m) {\n  return countsFeature(m.year, m.path);\n});\nvar fc = ee.FeatureCollection(feats).sort('year');\nprint('Pixel counts per class & year', fc);\n\n/**** Plot evolution ****/\nvar chart = ui.Chart.feature.byFeature(fc, 'year', ['class0', 'class1', 'class2', 'class3'])\n  .setChartType('LineChart')\n  .setOptions({\n    title: 'Bamako class pixel counts over time',\n    hAxis: {title: 'Year', format: '####'},\n    vAxis: {title: 'Pixel count'},\n    lineWidth: 2,\n    pointSize: 5,\n    series: {\n      0: {color: palette[0], labelInLegend: '0 water'},\n      1: {color: palette[1], labelInLegend: '1 veg'},\n      2: {color: palette[2], labelInLegend: '2 bare'},\n      3: {color: palette[3], labelInLegend: '3 builtup'}\n    }\n  });\nprint(chart);\n\n/**** Optional: map preview for first & last mosaic ****/\nvar first = mosaics[0];\nvar firstCls = ee.Image(first.path).select(bands).classify(classifier).clip(roi);\n\n\nvar last = mosaics[mosaics.length - 1];\nvar latestCls = ee.Image(last.path).select(bands).classify(classifier).clip(roi);\n\nMap.centerObject(roi, 10);\nMap.addLayer(roi, {}, 'ROI', false);\nMap.addLayer(firstCls, {min: 0, max: 3, palette: palette}, 'Classified ' + first.year);\nMap.addLayer(latestCls, {min: 0, max: 3, palette: palette}, 'Classified ' + last.year);\n\n\n\n. In Figure 1, I show the evolution of Bamako in Mali. The curve shows a fairly constant growth between 1975 and 2020, growing from around 60 square km to more than 140 square km, but the growth appears to flatten after 2020. Note that this could be an effect of the relatively small area we’re investigating (~20 km) becoming saturated and growth taking place elsewhere outside of our study area.\nWe can also look at the spatial distribution of the growth (see Figure 2). Clearly, Bamako has continue growing along the Niger River, but also outside from it, and we can see how the build up area ends up making a continuum between the main Bamako city and the towns that existed around it in 2000.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in the particular case of Bamako, you can read (Keita et al. (2021)). This paper does something similar to what you have done here.\n\n\n\n\n\n\n\n\nFigure 1: Evolution of the size of build up area for a radial region centred in Bamako (Mali) and extending 20km.\n\n\n\n\n\n\n\n\n\nFigure 2: Built up extent of Bamako in 2000 (red) and 2025 (blue)\n\n\n\n\n\n\nWhat you’ve got through is a typical classification problem. The goal of the classification (e.g.the “labels” you are interested in) and the input data might be different, but generally speaking, this is the blueprint. There are of course degrees of sophistication you could improve on. For example, in some tasks, the dynamics of the change are what is important. For example, if you are trying to map different types of trees, it might be useful to consider features in summer and winter, to easily separate evergreen and deciduous. Another refinement (specially if your data has a very fine spatial resolution) is to exploit the spatial context: in this practical, we have just taken pixels on their own, but pixels are often spatially correlated, so taking each pixel considering its neighbouring spatial context is a great idea. This is part of what makes so-called deep learning approaches so effective.\nAs EO measurements are quite indirect to what you’re generally after, it pays to spend time removing additional sources of variance in the data, and pre-processing the data to emphasise the characteristics that are useful to your problem. Of course, properly phrasing your problem is also required (do you need to retrieve 15 landcover classes, or are you after one or two?). So simplifying the problem and selecting input features that are strongly linked to you phenomenon of study is generally a good idea.\nThe indirect nature of the problem, augmented by the black box nature of most ML/classification algorithms is a risky prospect: plausible looking results are easy to get, but you need to build certainty on them. This is the role of the validation step: use an independent data set to test the classifier and assess its performance. Do not just consider one single metric, but report a broad suite of metrics, and use them to understand what is confusing the classifier. This exercise can guide you in refining the features you use. For example, in our study case, bare soils, vegetation and built up were confused to different degrees. Since we chose to use data from the dry season, we might be looking at vegetation that is very sparse, and mostly looks like soil. Were this an issue, adding perhaps a second wet season composite might help.\nCollecting training data is fundamental, as it allows you to actually select samples of your requirements and match them up with the EO data. You should take a lot of care in selecting it, and you should err in the side of caution: if unsure, collect more training data. You can always use it to validate things if you don’t use it for training. Ensure that your training set explores the true variability of the classes, and be aware that neighbouring pixels are strongly correlated, so prefer picking single points rather than large polygons. Stratify your sampling, and ensure you collect samples all over the region of interest (you don’t want to biased to a small region). If you’re doing multitemporal analyses like this one, you probably want to collect data at different periods, and covering different sensor configurations.\n\n\n\nPerhaps the most important lesson when thinking about using EO with your projects is to start thinking: “Surely, someone must have had a go at this before me?”. A large number of times, you will find that this is indeed the case, and that there is a large community of researchers that spend years finessing the best way of addressing the problem you’re trying to solve by creating “products”.\nProducts are often freely available, well documented and validated, and if widely used, will have their inadequacies or other “glitches” exposed. For the case at hand, we could have used the Global Human Settlement Layer (described in some detail in Pesaresi et al (2024)). This dataset extends EO data with detailed census data, and has been widely validated. Even if your final task is to use e.g., very high resolution optical data to map slums, it is well worth considering using established products to at least cross-compare or to add additional information that simplifies your task.",
    "crumbs": [
      "Download Full PDF",
      "5. Discussion"
    ]
  },
  {
    "objectID": "02-methods.html",
    "href": "02-methods.html",
    "title": "Data and Methods",
    "section": "",
    "text": "Even though the Landsat archive is free of charge and there are many ways of accessing it and using the data, here we will use Google Earth Engine.\n\n\n\n\n\n\nImportantGetting a GEE account\n\n\n\nYou will need a Google (or a “gmail”) account and then you can signup for GEE here. Follow the instructions therein\nNote that this process can take a few days to complete!!\n\n\nGEE is a cloud based solution, in which most of the heavy processing is done in Google’s servers. The processing is defined using a high level computing language (we’ll use javascript via the browser here, but Python is another possibility). Generally speaking, we will do most of the processing on GEE via the browser, but will export results (as e.g., raster files) that can then be further explored in other software tools. Figure 1 shows a labelled screenshot of GEE’s main editor window.\n\n\n\n\n\n\nFigure 1: The GEE code editor (from here)\n\n\n\n\n\n\n\n\nWe use Landsat surface reflectance data provided through Google Earth Engine. Landsat offers a unique, continuous global record of Earth observations from the 1980s to the present, making it particularly suitable for long-term urban studies. To ensure consistency across time, we combine data from multiple Landsat sensors:\n\nLandsat 5 TM (for earlier years)\nLandsat 7 ETM+\nLandsat 8 OLI\nLandsat 9 OLI-2\n\nAll datasets are taken from the Collection 2, Tier 1, Level-2 Surface Reflectance products. These data have already been radiometrically calibrated and atmospherically corrected, allowing surface reflectance values to be compared across sensors and years.\n\n\n\nThe analysis will be performed for the following years: 2000, 2005, 2010, 2015, 2020, and 2024 These snapshots provide a simple but effective way to visualise long-term urban expansion.\n\n\n\n\n\n\nImportant\n\n\n\nFigure 2 is the typical cloudiness in Bamako (from here)\n\n\n\n\n\n\nFigure 2: Bamako cloudiness/precipitation meteogram\n\n\n\nDoes that suggest anything useful in terms of creating an annual mosaic? Can yuou think of any period(s) that might be most useful and why?\n\n\n\n\n\nSatellite images are affected by clouds, cloud shadows, and other artefacts that can obscure the surface. To address this, we apply a quality mask based on the Landsat QA_PIXEL band. Pixels flagged as any of the following are removed:\n\nFill (missing data)\nDilated cloud\nCirrus cloud\nCloud\nCloud shadow\n\nIn addition, scenes are filtered using metadata to retain only images with less than 70% reported cloud cover. To keep processing efficient, no more than 25 images per year are used, prioritising the clearest scenes.\n\n\n\nDifferent Landsat sensors store spectral bands using different band numbers. To allow images from different missions to be combined:\nSpectral bands are renamed to a common set of names (blue, green, red, nir, swir1, swir2). Table 1 shows what these bands represent.\n\n\n\nTable 1: Common Landsat spectral bands\n\n\n\n\n\n\n\n\n\n\n\nBand name\nSpectral region\nApprox. wavelength (nm)\nWhat it is commonly used for\n\n\n\n\nblue\nBlue\n450–510\nWater, haze, urban features\n\n\ngreen\nGreen\n520–600\nVegetation reflectance, urban areas\n\n\nred\nRed\n630–690\nVegetation vs built-up contrast\n\n\nnir\nNear-infrared (NIR)\n760–900\nVegetation health, biomass\n\n\nswir1\nShortwave IR (SWIR)\n1550–1750\nBuilt-up areas, soil, moisture\n\n\nswir2\nShortwave IR (SWIR)\n2080–2350\nUrban materials, dryness, fire scars\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis harmonisation step ensures that reflectance values are directly comparable across sensors and years.\n\n\nSurface reflectance values are rescaled using the official Landsat Collection 2 scaling factors\n\n\n\n\n\n\n\n\n\nFigure 3: Sample spectral reflectance of vegetation, crops, built-up, bare soils and water classes (from DOI:10.1016/j.isprsjprs.2017.11.006\n\n\n\nSome simple manipulations of reflectance are useful to differentiate different land cover classes. For example, the large reflectance contrast between the nir and red bands in healthy vegetation (see Figure 3) can be used to detect its prevalence. Similar empirical relationships have been investigated for other scene types, such as urban areas or water. In Table 2 there’s brief summary of these\n\n\n\nTable 2: Useful empirical band combinations (indices)\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nFull name\nFormula (Landsat bands)\nWhat it highlights\nNotes\n\n\n\n\nNDBI\nNormalised Difference Built-up Index\n(swir1 − nir) / (swir1 + nir)\nBuilt-up / impervious surfaces\nSimple, widely used; can confuse bare soil\n\n\nUI\nUrban Index\n(swir2 − nir) / (swir2 + nir)\nDense urban areas\nStrong urban contrast; more noise-sensitive\n\n\nIBI\nIndex-based Built-up Index\n(NDBI − (NDVI + MNDWI)) / (NDBI + (NDVI + MNDWI))\nBuilt-up areas with vegetation and water suppressed\nMore complex but very effective\n\n\nEBBI\nEnhanced Built-up and Bareness Index\n(swir1 − nir) / (10 × √(swir1 + TIR))\nBuilt-up vs bare soil\nRequires thermal band; advanced\n\n\nNDVI\nNormalised Difference Vegetation Index\n(nir − red) / (nir + red)\nVegetation\nOften used as a vegetation mask, not an urban index\n\n\nMNDWI\nModified Normalised Difference Water Index\n(green − swir1) / (green + swir1)\nWater bodies\nUsed within IBI to suppress water\n\n\n\n\n\n\n\n\n\nFor each year, all selected and processed images are combined into a single annual composite using the median value of each pixel. Using the median:\n\nReduces the influence of residual clouds or outliers\nProduces a more stable and representative image of surface conditions\nIs well suited for urban studies where abrupt extremes are not desired\n\nEach annual composite is clipped to the AOI and assigned a timestamp corresponding to the study year.\n\n\n\nFor exploratory analysis, annual composites are displayed as true-colour (RGB) images using the red, green, and blue bands. A consistent reflectance stretch is applied across all years to ensure that visual differences primarily reflect real land surface change rather than display settings.\n\n\n\nIn this first stage of the practical, we:\n1 .Define an urban area of interest 2. Select multi-decadal Landsat surface reflectance data 3. Apply cloud masking and quality filtering 4. Harmonise spectral bands across sensors 5. Create annual composites with different feature sets\nThese annual mosaics form the foundation for later steps in the practical, where we will quantify and interpret urban growth patterns.\n\n\n\n\nTo map urban expansion, we use a supervised classification approach. In supervised classification, the algorithm “learns” how different land cover types appear in satellite data based on training samples provided by the user. In this practical, you can define three or four land cover classes (e.g., built-up areas, vegetation, bare soil, and water. Although all classes are included to improve discrimination, the primary focus of the analysis is on identifying built-up (urban) areas accurately.\n\n\n\n\n\n\nTip\n\n\n\nHaving additional classes can be useful if there are some uninteresting” classes are very distinct (e.g. water or vegetation) that can confuse the algorithm.\n\n\nTraining data are provided as a labelled feature collection, where each feature represents a location with a known land cover class. These training labels are typically created by manually digitising polygons or points using high-resolution imagery as reference. Each training feature contains a class identifier (e.g. an integer code) that links the sample to one of the four land cover types. Spectral values and indices (e.g. NDBI, NDVI) are extracted from the Landsat composites at these locations and used as input to the classifier.\nWe use a Random Forest (RF) classifier, which is an ensemble machine-learning method based on many decision trees. RF is well suited to remote sensing applications because it handles non-linear relationships, is relatively robust to noise, and performs well with a limited number of training samples. The labelled data are split into training and testing subsets, typically using a random partition (for example, 70% for training and 30% for testing). The training subset is used to build the model, while the testing subset provides an independent estimate of classification accuracy.\nModel performance can be roughly validated using a confusion matrix and overall accuracy derived from the testing data. Additional qualitative validation is encouraged by visually comparing the classified map with true-colour imagery and checking whether built-up areas correspond to known urban features such as dense neighbourhoods and road networks. This combination of quantitative and visual checks will helpyou assess the suitability of your results and whether they are fit for purpose.",
    "crumbs": [
      "Download Full PDF",
      "2. Methodology"
    ]
  },
  {
    "objectID": "02-methods.html#data-and-methods",
    "href": "02-methods.html#data-and-methods",
    "title": "Data and Methods",
    "section": "",
    "text": "Even though the Landsat archive is free of charge and there are many ways of accessing it and using the data, here we will use Google Earth Engine.\n\n\n\n\n\n\nImportantGetting a GEE account\n\n\n\nYou will need a Google (or a “gmail”) account and then you can signup for GEE here. Follow the instructions therein\nNote that this process can take a few days to complete!!\n\n\nGEE is a cloud based solution, in which most of the heavy processing is done in Google’s servers. The processing is defined using a high level computing language (we’ll use javascript via the browser here, but Python is another possibility). Generally speaking, we will do most of the processing on GEE via the browser, but will export results (as e.g., raster files) that can then be further explored in other software tools. Figure 1 shows a labelled screenshot of GEE’s main editor window.\n\n\n\n\n\n\nFigure 1: The GEE code editor (from here)\n\n\n\n\n\n\n\n\nWe use Landsat surface reflectance data provided through Google Earth Engine. Landsat offers a unique, continuous global record of Earth observations from the 1980s to the present, making it particularly suitable for long-term urban studies. To ensure consistency across time, we combine data from multiple Landsat sensors:\n\nLandsat 5 TM (for earlier years)\nLandsat 7 ETM+\nLandsat 8 OLI\nLandsat 9 OLI-2\n\nAll datasets are taken from the Collection 2, Tier 1, Level-2 Surface Reflectance products. These data have already been radiometrically calibrated and atmospherically corrected, allowing surface reflectance values to be compared across sensors and years.\n\n\n\nThe analysis will be performed for the following years: 2000, 2005, 2010, 2015, 2020, and 2024 These snapshots provide a simple but effective way to visualise long-term urban expansion.\n\n\n\n\n\n\nImportant\n\n\n\nFigure 2 is the typical cloudiness in Bamako (from here)\n\n\n\n\n\n\nFigure 2: Bamako cloudiness/precipitation meteogram\n\n\n\nDoes that suggest anything useful in terms of creating an annual mosaic? Can yuou think of any period(s) that might be most useful and why?\n\n\n\n\n\nSatellite images are affected by clouds, cloud shadows, and other artefacts that can obscure the surface. To address this, we apply a quality mask based on the Landsat QA_PIXEL band. Pixels flagged as any of the following are removed:\n\nFill (missing data)\nDilated cloud\nCirrus cloud\nCloud\nCloud shadow\n\nIn addition, scenes are filtered using metadata to retain only images with less than 70% reported cloud cover. To keep processing efficient, no more than 25 images per year are used, prioritising the clearest scenes.\n\n\n\nDifferent Landsat sensors store spectral bands using different band numbers. To allow images from different missions to be combined:\nSpectral bands are renamed to a common set of names (blue, green, red, nir, swir1, swir2). Table 1 shows what these bands represent.\n\n\n\nTable 1: Common Landsat spectral bands\n\n\n\n\n\n\n\n\n\n\n\nBand name\nSpectral region\nApprox. wavelength (nm)\nWhat it is commonly used for\n\n\n\n\nblue\nBlue\n450–510\nWater, haze, urban features\n\n\ngreen\nGreen\n520–600\nVegetation reflectance, urban areas\n\n\nred\nRed\n630–690\nVegetation vs built-up contrast\n\n\nnir\nNear-infrared (NIR)\n760–900\nVegetation health, biomass\n\n\nswir1\nShortwave IR (SWIR)\n1550–1750\nBuilt-up areas, soil, moisture\n\n\nswir2\nShortwave IR (SWIR)\n2080–2350\nUrban materials, dryness, fire scars\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis harmonisation step ensures that reflectance values are directly comparable across sensors and years.\n\n\nSurface reflectance values are rescaled using the official Landsat Collection 2 scaling factors\n\n\n\n\n\n\n\n\n\nFigure 3: Sample spectral reflectance of vegetation, crops, built-up, bare soils and water classes (from DOI:10.1016/j.isprsjprs.2017.11.006\n\n\n\nSome simple manipulations of reflectance are useful to differentiate different land cover classes. For example, the large reflectance contrast between the nir and red bands in healthy vegetation (see Figure 3) can be used to detect its prevalence. Similar empirical relationships have been investigated for other scene types, such as urban areas or water. In Table 2 there’s brief summary of these\n\n\n\nTable 2: Useful empirical band combinations (indices)\n\n\n\n\n\n\n\n\n\n\n\n\nIndex\nFull name\nFormula (Landsat bands)\nWhat it highlights\nNotes\n\n\n\n\nNDBI\nNormalised Difference Built-up Index\n(swir1 − nir) / (swir1 + nir)\nBuilt-up / impervious surfaces\nSimple, widely used; can confuse bare soil\n\n\nUI\nUrban Index\n(swir2 − nir) / (swir2 + nir)\nDense urban areas\nStrong urban contrast; more noise-sensitive\n\n\nIBI\nIndex-based Built-up Index\n(NDBI − (NDVI + MNDWI)) / (NDBI + (NDVI + MNDWI))\nBuilt-up areas with vegetation and water suppressed\nMore complex but very effective\n\n\nEBBI\nEnhanced Built-up and Bareness Index\n(swir1 − nir) / (10 × √(swir1 + TIR))\nBuilt-up vs bare soil\nRequires thermal band; advanced\n\n\nNDVI\nNormalised Difference Vegetation Index\n(nir − red) / (nir + red)\nVegetation\nOften used as a vegetation mask, not an urban index\n\n\nMNDWI\nModified Normalised Difference Water Index\n(green − swir1) / (green + swir1)\nWater bodies\nUsed within IBI to suppress water\n\n\n\n\n\n\n\n\n\nFor each year, all selected and processed images are combined into a single annual composite using the median value of each pixel. Using the median:\n\nReduces the influence of residual clouds or outliers\nProduces a more stable and representative image of surface conditions\nIs well suited for urban studies where abrupt extremes are not desired\n\nEach annual composite is clipped to the AOI and assigned a timestamp corresponding to the study year.\n\n\n\nFor exploratory analysis, annual composites are displayed as true-colour (RGB) images using the red, green, and blue bands. A consistent reflectance stretch is applied across all years to ensure that visual differences primarily reflect real land surface change rather than display settings.\n\n\n\nIn this first stage of the practical, we:\n1 .Define an urban area of interest 2. Select multi-decadal Landsat surface reflectance data 3. Apply cloud masking and quality filtering 4. Harmonise spectral bands across sensors 5. Create annual composites with different feature sets\nThese annual mosaics form the foundation for later steps in the practical, where we will quantify and interpret urban growth patterns.\n\n\n\n\nTo map urban expansion, we use a supervised classification approach. In supervised classification, the algorithm “learns” how different land cover types appear in satellite data based on training samples provided by the user. In this practical, you can define three or four land cover classes (e.g., built-up areas, vegetation, bare soil, and water. Although all classes are included to improve discrimination, the primary focus of the analysis is on identifying built-up (urban) areas accurately.\n\n\n\n\n\n\nTip\n\n\n\nHaving additional classes can be useful if there are some uninteresting” classes are very distinct (e.g. water or vegetation) that can confuse the algorithm.\n\n\nTraining data are provided as a labelled feature collection, where each feature represents a location with a known land cover class. These training labels are typically created by manually digitising polygons or points using high-resolution imagery as reference. Each training feature contains a class identifier (e.g. an integer code) that links the sample to one of the four land cover types. Spectral values and indices (e.g. NDBI, NDVI) are extracted from the Landsat composites at these locations and used as input to the classifier.\nWe use a Random Forest (RF) classifier, which is an ensemble machine-learning method based on many decision trees. RF is well suited to remote sensing applications because it handles non-linear relationships, is relatively robust to noise, and performs well with a limited number of training samples. The labelled data are split into training and testing subsets, typically using a random partition (for example, 70% for training and 30% for testing). The training subset is used to build the model, while the testing subset provides an independent estimate of classification accuracy.\nModel performance can be roughly validated using a confusion matrix and overall accuracy derived from the testing data. Additional qualitative validation is encouraged by visually comparing the classified map with true-colour imagery and checking whether built-up areas correspond to known urban features such as dense neighbourhoods and road networks. This combination of quantitative and visual checks will helpyou assess the suitability of your results and whether they are fit for purpose.",
    "crumbs": [
      "Download Full PDF",
      "2. Methodology"
    ]
  },
  {
    "objectID": "03-code.html",
    "href": "03-code.html",
    "title": "Creation of annual composites in GEE",
    "section": "",
    "text": "This is the actual code that performs this. Most of the code is standard boilerplate, so it’s fundamentally a “cut and paste” job, but you should change options and see the results! While all these different snippets are given individually, you should copy them in sequence in the code editor.\nI will use using Bamako for this example, but you’ll have to change this to other cities!\n\n\nIt’s a good idea to define options and variables once, and keep referring to them. If you want to change things later on, you can do this in a managed way.\n/**************************************************************\n * SETTINGS (you can edit these)\n **************************************************************/\n\n// City and AOI (example: Bamako)\nvar cityName = 'Bamako';\nvar cityPoint = ee.Geometry.Point([-8.0029, 12.6392]);\n\n// AOI size: buffer radius in metres \nvar aoiRadiusMeters = 20000;\n\n// Target years for the practical\nvar targetYears = [2000, 2005, 2010, 2015, 2020, 2024];\n\n// Choose an annual period to calculate the mosaic\nvar seasonStartMonth = 1;  // January\nvar seasonEndMonth   = 4;  // April (end is handled as May 1 below)\n\n// Scene filtering knobs (speed + robustness)\nvar maxCloudCoverPercent = 70; // pre-filter scenes by metadata cloud cover\nvar maxScenesPerYear = 25;     // keep only the clearest N scenes (big speed win)\n\n// Visualisation stretch for reflectance RGB/false colour (scaled reflectance ~0–1)\nvar rgbVis = {min: 0.0, max: 0.30};\n\n\n\nWe will also select our region (simply my creating a circular buffer around the centre lat/lon of Bamako)\n/**************************************************************\n * MAP SETUP (AOI, quick sanity check)\n **************************************************************/\n\nvar aoi = cityPoint.buffer(aoiRadiusMeters); // simple circular AOI (robust geometry)\n\nMap.centerObject(aoi, 12); // 12 is the initial zoom level\nMap.addLayer(aoi, {color: 'yellow'}, cityName + ' AOI');\n\n// AOI area sanity check (needs a small maxError)\nprint('AOI area (km^2):', aoi.area(1).divide(1e6));\n\n\n\nThe following snippet selects the different Landsat collections (we need different sensors to cover our large temporal period). The Landsat L2 collection has a per pixel field callec QA_PIXEL that encodes whether a cloud or shadow has been detected. It also uses a dilated mask to try to mask cloud shadows. We then need to scale the data to units of reflectance (a number between 0 and 1), and additionally, we remove (mask) any pixels where the reflectance is outside this range (can happen due to to processing issues), or whether any of the bands are missing.\n/**************************************************************\n * LOAD RAW LANDSAT COLLECTIONS (Collection 2, Level-2)\n *\n * We keep these “raw” collections separate from preprocessing.\n * Important idea: filter on metadata first, then map() preprocessing.\n **************************************************************/\n\nvar landsat5_raw = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'); // Landsat 5 TM\nvar landsat7_raw = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'); // Landsat 7 ETM+\nvar landsat8_raw = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2'); // Landsat 8 OLI\nvar landsat9_raw = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'); // Landsat 9 OLI-2\n\n\n/**************************************************************\n * CLOUD/SHADOW MASKING (using QA_PIXEL)\n *\n * We use QA_PIXEL bits to remove:\n *  - Fill\n *  - Dilated cloud\n *  - Cirrus\n *  - Cloud\n *  - Cloud shadow\n *\n * This is deliberately “simple but decent” for a taster practical.\n **************************************************************/\n\nfunction qaCloudShadowMask_C2L2(image) {\n  var qa = image.select('QA_PIXEL');\n\n  // Build a single-band boolean mask:\n  // 1 means “keep”, 0 means “mask out”\n  var mask = qa.bitwiseAnd(1 &lt;&lt; 0).eq(0)   // not fill\n    .and(qa.bitwiseAnd(1 &lt;&lt; 1).eq(0))      // not dilated cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 2).eq(0))      // not cirrus\n    .and(qa.bitwiseAnd(1 &lt;&lt; 3).eq(0))      // not cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 4).eq(0));     // not cloud shadow\n\n  return mask;\n}\n\n\n/**************************************************************\n * REFLECTANCE SCALING + \"FEATURE HYGIENE\"\n *\n * Landsat C2 L2 SR scaling:\n *   SR = DN * 0.0000275 + (-0.2)\n *\n * \n *  - Clamp reflectance to [0, 1] to avoid odd outliers.\n *  - Mask pixels where any band is missing (edges, artefacts).\n **************************************************************/\n\nfunction scaleAndCleanReflectance(srImage) {\n  // Apply the scaling\n  var scaled = srImage.multiply(0.0000275).add(-0.2);\n\n  // Clamp to a sensible range\n  scaled = scaled.clamp(0, 1);\n\n  // Mask out pixels where any band is missing/invalid\n  // (reduce(min) checks for missing values across bands)\n  var allBandsPresent = scaled.reduce(ee.Reducer.min()).gte(0);\n  return scaled.updateMask(allBandsPresent);\n}\n\n\n\nThe different Landsat sensors have different band names. Here, we relabel them to a common set of names. We use the previous code snippets to apply the cloud mask, reflectance scaling and additional cleaning up to the data.\n/**************************************************************\n * SENSOR HARMONISATION (common band names)\n *\n * Landsat 5/7 SR bands: SR_B1..SR_B5, SR_B7 (Band 6 is thermal ST_B6)\n * Landsat 8/9 SR bands: SR_B2..SR_B7 (Band 1 is coastal; we ignore it)\n *\n * We rename to a common 6-band set:\n *   blue, green, red, nir, swir1, swir2\n **************************************************************/\n\nfunction prepLandsat57(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B1','SR_B2','SR_B3','SR_B4','SR_B5','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\nfunction prepLandsat89(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\n\n\nIn many applications, spectral band combinations can be useful to enhance the signal and facilitate classification. They’re also quite useful to visually interpret images qualitatively, so we’ll add a bunch of them here.\n/**************************************************************\n * ADD INDICES (extra feature bands for classification)\n *\n * Indices included:\n *  - NDVI  (vegetation)\n *  - MNDWI (water)\n *  - NDBI  (built-up proxy)\n *  - UI    (simple urban index): UI = NDBI - NDVI (easy to explain)\n *  - IBI   (index-based built-up index): contrasts built-up vs veg+water\n *  - BRIGHT (optional): mean reflectance as a simple brightness feature\n *\n * Note: There are multiple definitions in the literature;\n **************************************************************/\n\nfunction safeDivide(numerator, denominator) {\n  // Avoid divide-by-zero (if den == 0, set it to 1)\n  denominator = denominator.where(denominator.eq(0), 1);\n  return numerator.divide(denominator);\n}\n\nfunction addFeatureIndices(img) {\n  // These assume our common bands exist: red, green, nir, swir1\n  var ndvi  = img.normalizedDifference(['nir', 'red']).rename('NDVI');\n  var mndwi = img.normalizedDifference(['green', 'swir1']).rename('MNDWI');\n  var ndbi  = img.normalizedDifference(['swir1', 'nir']).rename('NDBI');\n\n  // Simple urban index (teachable): built-up proxy minus vegetation\n  var ui = ndbi.subtract(ndvi).rename('UI');\n\n  // IBI (common formulation)\n  var avgVegWater = ndvi.add(mndwi).divide(2);\n  var ibi = safeDivide(ndbi.subtract(avgVegWater), ndbi.add(avgVegWater)).rename('IBI');\n\n  // Brightness: average reflectance (very intuitive for students)\n  var bright = img.select(['blue','green','red','nir','swir1','swir2'])\n    .reduce(ee.Reducer.mean())\n    .rename('BRIGHT');\n\n  return img.addBands([ndvi, mndwi, ndbi, ui, ibi, bright]);\n}\n\n\n\nNow, all the stuff we defined above will start to get used. We need to build a stack of cleaned images per year that cover our region of interest, are masked for clouds etc. For years before 2013, we’ll use Landsat 5 and 7, for years afterwards, we’ll use Landsat 8 and 9. Note that we’re using the variables defined at the top to control things like maximum cloud coverage etc.\n/**************************************************************\n * BUILD A PER-YEAR IMAGE COLLECTION (filter first, then map)\n *\n * Steps:\n *  - Choose a sensor group depending on year (&lt;=2012 uses L5+L7; &gt;=2013 uses L8+L9)\n *  - Filter by AOI and dry-season date window\n *  - Filter by CLOUD_COVER metadata, sort clearest-first, keep top N scenes\n *  - Map preprocessing (mask + scale + rename bands)\n **************************************************************/\n\nfunction getSeasonDateRange(year) {\n  year = ee.Number(year);\n\n  var start = ee.Date.fromYMD(year, seasonStartMonth, 1);\n\n  // endMonth is inclusive-ish: we use the first day of the next month as end\n  var end = ee.Date.fromYMD(year, seasonEndMonth + 1, 1);\n\n  return {start: start, end: end};\n}\n\nfunction collectionForYear(year) {\n  year = ee.Number(year);\n  var range = getSeasonDateRange(year);\n\n  // Choose sensors based on era\n  var rawCollection = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      landsat5_raw.merge(landsat7_raw),   // 2000–2012 era\n      landsat8_raw.merge(landsat9_raw)    // 2013+ era\n    )\n  );\n\n  // Filter on raw metadata BEFORE preprocessing\n  var filtered = rawCollection\n    .filterBounds(aoi)\n    .filterDate(range.start, range.end)\n    .filterMetadata('CLOUD_COVER', 'less_than', maxCloudCoverPercent)\n    .sort('CLOUD_COVER')\n    .limit(maxScenesPerYear);\n\n  // Now map preprocessing and band harmonisation\n  var prepped = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      filtered.map(prepLandsat57),\n      filtered.map(prepLandsat89)\n    )\n  );\n\n  return prepped;\n}\n\n\n\nYou thought we’d never get here! The next bit defines how the composite gets done (using the median), adds the relevant indices and applies it to all the years.\n/**************************************************************\n * ANNUAL COMPOSITE (median) + indices\n *\n * The “annual mosaic” is the median of all (masked) images in the season.\n * Then we add indices so the final image is ready for classification.\n **************************************************************/\n\nfunction annualComposite(year) {\n  year = ee.Number(year);\n\n  var col = collectionForYear(year);\n\n  // Diagnostics: useful for teaching and debugging\n  print('Year', year, 'images used:', col.size());\n\n  // Build the composite and add feature indices\n  var composite = addFeatureIndices(col.median())\n    .clip(aoi)\n    .set('year', year)\n    // time_start used later if we chart time series\n    .set('system:time_start', ee.Date.fromYMD(year, 3, 1).millis());\n\n  return composite;\n}\n\n// Build mosaics for all target years\nvar annualMosaics = ee.ImageCollection(targetYears.map(annualComposite));\nprint('Annual mosaics:', annualMosaics);\n\n\n\n/**************************************************************\n * DISPLAY (choose one year to show)\n *\n * Show:\n *  - RGB (natural colour)\n *  - False colour (NIR / red / green) helps see vegetation strongly\n *  - Indices (NDVI, MNDWI, NDBI, UI, IBI)\n *\n * Note: we use visualize() for the RGB layers because it makes map\n * rendering snappier in the Code Editor.\n **************************************************************/\n\nvar displayYear = 2020;\nvar mosaicToShow = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', displayYear)).first()\n);\n\n// Always sanity-check the bands (helpful if something returns empty)\nprint('Bands in displayed mosaic:', mosaicToShow.bandNames());\n\n// RGB and false colour previews\nMap.addLayer(\n  mosaicToShow.select(['red','green','blue']).visualize(rgbVis),\n  {},\n  'RGB ' + displayYear\n);\n\nMap.addLayer(\n  mosaicToShow.select(['nir','red','green']).visualize(rgbVis),\n  {},\n  'False colour (NIR/R/G) ' + displayYear\n);\n\n// Index layers (simple min/max for teaching; no palettes needed)\nMap.addLayer(mosaicToShow.select('NDVI'),  {min: -0.2, max: 0.8}, 'NDVI');\nMap.addLayer(mosaicToShow.select('MNDWI'), {min: -0.6, max: 0.6}, 'MNDWI');\nMap.addLayer(mosaicToShow.select('NDBI'),  {min: -0.6, max: 0.6}, 'NDBI');\nMap.addLayer(mosaicToShow.select('UI'),    {min: -1.0, max: 1.0}, 'UI (= NDBI - NDVI)');\nMap.addLayer(mosaicToShow.select('IBI'),   {min: -1.0, max: 1.0}, 'IBI');\nMap.addLayer(mosaicToShow.select('BRIGHT'),{min: 0.0,  max: 0.4}, 'Brightness');\n\n\n\nFor repeatibility and further analysis, we’ll export the data to your Google Drive. We’ll use the standard GeoTIFF format, with internal compression to make the data more manageable. This snippet exports a single year, make sure you’re happy with it before exporting all the years.\n/**************************************************************\n * EXPORT ONE YEAR MOSAIC TO GOOGLE DRIVE\n *\n * Pattern:\n *  - Export ONE year first (students learn exports without waiting ages)\n *  - Later, export all years or the classification outputs\n **************************************************************/\n\n// Uncomment to export the displayed year mosaic (multiband)\n \nExport.image.toDrive({\n  image: mosaicToShow, // includes SR bands + indices\n  description: cityName + '_Mosaic_' + displayYear,\n  folder: 'GEE_exports',\n  fileNamePrefix: cityName + '_Mosaic_' + displayYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13,\n  fileFormat: 'GeoTIFF',\n  formatOptions: {\n    cloudOptimized: true,   // Makes it a COG\n    compression: 'DEFLATE' // Powerful lossless compression\n  }\n});\n\n\n\n\n\nOnce you’re happy with the single year export, it’s time to expoert all the years in one go.\n/**************************************************************\n * EXPORT ALL YEARS (one multiband GeoTIFF per year)\n * - Exports each annual mosaic (SR bands + indices) to Google Drive\n * - GeoTIFF options: cloud-optimised (COG) + DEFLATE compression\n **************************************************************/\n\n// Folder in Google Drive\nvar exportFolder = 'GEE_exports';\n\n// Choose a scale appropriate for Landsat SR\nvar exportScale = 30;\n\n// Loop over years and create one export task per year\ntargetYears.forEach(function(year) {\n\n  var img = ee.Image(\n    annualMosaics.filter(ee.Filter.eq('year', year)).first()\n  );\n\n  Export.image.toDrive({\n    image: img,                       // multiband: SR + indices\n    description: cityName + '_Mosaic_' + year,\n    folder: exportFolder,\n    fileNamePrefix: cityName + '_Mosaic_' + year,\n    region: aoi,\n    scale: exportScale,\n    maxPixels: 1e13,\n    fileFormat: 'GeoTIFF',\n    formatOptions: {\n      cloudOptimized: true,\n      compression: 'DEFLATE'\n    }\n  });\n\n});\n\n\n\nSo far, we have used Google Earth Engine to build annual mosaics for our city of interest. These mosaics exist only temporarily inside the script unless we explicitly save them. In addition to exporting images to Google Drive, Earth Engine allows us to export data as Assets, which are stored permanently within our Earth Engine account. Saving the mosaics as Assets is useful because they can be imported directly into other scripts without re-running the mosaicking workflow, making it easier to separate data preparation from later analysis steps such as classification or time-series analysis.\nTo export an annual mosaic as an Asset, we select one of the mosaics from our annualMosaics ImageCollection and use Export.image.toAsset(). When doing this, we must specify:\n\nan asset ID, which defines where the image will be stored in our Earth Engine Assets,\nthe region to export (our AOI),\nthe spatial resolution (30 m for Landsat data).\n\nThe example below exports the mosaic for a single year. Once the export task has completed, the image will appear in the Assets tab of the Earth Engine Code Editor and can be loaded in other scripts using its asset path.\n// Select the mosaic for the year we want to export\nvar exportYear = 2020;\n\nvar mosaicToExport = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', exportYear)).first()\n);\n\n// Export the mosaic as a Google Earth Engine Asset\nExport.image.toAsset({\n  image: mosaicToExport,                 // multiband image (SR + indices)\n  description: cityName + '_Mosaic_' + exportYear,\n  assetId: 'users/YOUR_USERNAME/' + cityName + '_Mosaic_' + exportYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13\n}); // Change YOUR_USERNAME with your own GEE username!\n\n\n\n\n\n\nWarning\n\n\n\nAfter starting the export, the task will appear in the Tasks tab of the Code Editor. You must manually click Run to begin the export. When it has finished, the mosaic can be reused in other Earth Engine scripts by loading it from the Assets panel, allowing you to work directly with the prepared annual composites without repeating the earlier processing steps.",
    "crumbs": [
      "Download Full PDF",
      "3. Create annual Landsat composites"
    ]
  },
  {
    "objectID": "03-code.html#creating-an-annual-composite-in-gee",
    "href": "03-code.html#creating-an-annual-composite-in-gee",
    "title": "Creation of annual composites in GEE",
    "section": "",
    "text": "This is the actual code that performs this. Most of the code is standard boilerplate, so it’s fundamentally a “cut and paste” job, but you should change options and see the results! While all these different snippets are given individually, you should copy them in sequence in the code editor.\nI will use using Bamako for this example, but you’ll have to change this to other cities!\n\n\nIt’s a good idea to define options and variables once, and keep referring to them. If you want to change things later on, you can do this in a managed way.\n/**************************************************************\n * SETTINGS (you can edit these)\n **************************************************************/\n\n// City and AOI (example: Bamako)\nvar cityName = 'Bamako';\nvar cityPoint = ee.Geometry.Point([-8.0029, 12.6392]);\n\n// AOI size: buffer radius in metres \nvar aoiRadiusMeters = 20000;\n\n// Target years for the practical\nvar targetYears = [2000, 2005, 2010, 2015, 2020, 2024];\n\n// Choose an annual period to calculate the mosaic\nvar seasonStartMonth = 1;  // January\nvar seasonEndMonth   = 4;  // April (end is handled as May 1 below)\n\n// Scene filtering knobs (speed + robustness)\nvar maxCloudCoverPercent = 70; // pre-filter scenes by metadata cloud cover\nvar maxScenesPerYear = 25;     // keep only the clearest N scenes (big speed win)\n\n// Visualisation stretch for reflectance RGB/false colour (scaled reflectance ~0–1)\nvar rgbVis = {min: 0.0, max: 0.30};\n\n\n\nWe will also select our region (simply my creating a circular buffer around the centre lat/lon of Bamako)\n/**************************************************************\n * MAP SETUP (AOI, quick sanity check)\n **************************************************************/\n\nvar aoi = cityPoint.buffer(aoiRadiusMeters); // simple circular AOI (robust geometry)\n\nMap.centerObject(aoi, 12); // 12 is the initial zoom level\nMap.addLayer(aoi, {color: 'yellow'}, cityName + ' AOI');\n\n// AOI area sanity check (needs a small maxError)\nprint('AOI area (km^2):', aoi.area(1).divide(1e6));\n\n\n\nThe following snippet selects the different Landsat collections (we need different sensors to cover our large temporal period). The Landsat L2 collection has a per pixel field callec QA_PIXEL that encodes whether a cloud or shadow has been detected. It also uses a dilated mask to try to mask cloud shadows. We then need to scale the data to units of reflectance (a number between 0 and 1), and additionally, we remove (mask) any pixels where the reflectance is outside this range (can happen due to to processing issues), or whether any of the bands are missing.\n/**************************************************************\n * LOAD RAW LANDSAT COLLECTIONS (Collection 2, Level-2)\n *\n * We keep these “raw” collections separate from preprocessing.\n * Important idea: filter on metadata first, then map() preprocessing.\n **************************************************************/\n\nvar landsat5_raw = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'); // Landsat 5 TM\nvar landsat7_raw = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'); // Landsat 7 ETM+\nvar landsat8_raw = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2'); // Landsat 8 OLI\nvar landsat9_raw = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'); // Landsat 9 OLI-2\n\n\n/**************************************************************\n * CLOUD/SHADOW MASKING (using QA_PIXEL)\n *\n * We use QA_PIXEL bits to remove:\n *  - Fill\n *  - Dilated cloud\n *  - Cirrus\n *  - Cloud\n *  - Cloud shadow\n *\n * This is deliberately “simple but decent” for a taster practical.\n **************************************************************/\n\nfunction qaCloudShadowMask_C2L2(image) {\n  var qa = image.select('QA_PIXEL');\n\n  // Build a single-band boolean mask:\n  // 1 means “keep”, 0 means “mask out”\n  var mask = qa.bitwiseAnd(1 &lt;&lt; 0).eq(0)   // not fill\n    .and(qa.bitwiseAnd(1 &lt;&lt; 1).eq(0))      // not dilated cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 2).eq(0))      // not cirrus\n    .and(qa.bitwiseAnd(1 &lt;&lt; 3).eq(0))      // not cloud\n    .and(qa.bitwiseAnd(1 &lt;&lt; 4).eq(0));     // not cloud shadow\n\n  return mask;\n}\n\n\n/**************************************************************\n * REFLECTANCE SCALING + \"FEATURE HYGIENE\"\n *\n * Landsat C2 L2 SR scaling:\n *   SR = DN * 0.0000275 + (-0.2)\n *\n * \n *  - Clamp reflectance to [0, 1] to avoid odd outliers.\n *  - Mask pixels where any band is missing (edges, artefacts).\n **************************************************************/\n\nfunction scaleAndCleanReflectance(srImage) {\n  // Apply the scaling\n  var scaled = srImage.multiply(0.0000275).add(-0.2);\n\n  // Clamp to a sensible range\n  scaled = scaled.clamp(0, 1);\n\n  // Mask out pixels where any band is missing/invalid\n  // (reduce(min) checks for missing values across bands)\n  var allBandsPresent = scaled.reduce(ee.Reducer.min()).gte(0);\n  return scaled.updateMask(allBandsPresent);\n}\n\n\n\nThe different Landsat sensors have different band names. Here, we relabel them to a common set of names. We use the previous code snippets to apply the cloud mask, reflectance scaling and additional cleaning up to the data.\n/**************************************************************\n * SENSOR HARMONISATION (common band names)\n *\n * Landsat 5/7 SR bands: SR_B1..SR_B5, SR_B7 (Band 6 is thermal ST_B6)\n * Landsat 8/9 SR bands: SR_B2..SR_B7 (Band 1 is coastal; we ignore it)\n *\n * We rename to a common 6-band set:\n *   blue, green, red, nir, swir1, swir2\n **************************************************************/\n\nfunction prepLandsat57(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B1','SR_B2','SR_B3','SR_B4','SR_B5','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\nfunction prepLandsat89(image) {\n  var mask = qaCloudShadowMask_C2L2(image);\n\n  var sr = image.select(\n    ['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7'],\n    ['blue','green','red','nir','swir1','swir2']\n  );\n\n  var cleaned = scaleAndCleanReflectance(sr)\n    .updateMask(mask)\n    .copyProperties(image, image.propertyNames());\n\n  return cleaned;\n}\n\n\n\nIn many applications, spectral band combinations can be useful to enhance the signal and facilitate classification. They’re also quite useful to visually interpret images qualitatively, so we’ll add a bunch of them here.\n/**************************************************************\n * ADD INDICES (extra feature bands for classification)\n *\n * Indices included:\n *  - NDVI  (vegetation)\n *  - MNDWI (water)\n *  - NDBI  (built-up proxy)\n *  - UI    (simple urban index): UI = NDBI - NDVI (easy to explain)\n *  - IBI   (index-based built-up index): contrasts built-up vs veg+water\n *  - BRIGHT (optional): mean reflectance as a simple brightness feature\n *\n * Note: There are multiple definitions in the literature;\n **************************************************************/\n\nfunction safeDivide(numerator, denominator) {\n  // Avoid divide-by-zero (if den == 0, set it to 1)\n  denominator = denominator.where(denominator.eq(0), 1);\n  return numerator.divide(denominator);\n}\n\nfunction addFeatureIndices(img) {\n  // These assume our common bands exist: red, green, nir, swir1\n  var ndvi  = img.normalizedDifference(['nir', 'red']).rename('NDVI');\n  var mndwi = img.normalizedDifference(['green', 'swir1']).rename('MNDWI');\n  var ndbi  = img.normalizedDifference(['swir1', 'nir']).rename('NDBI');\n\n  // Simple urban index (teachable): built-up proxy minus vegetation\n  var ui = ndbi.subtract(ndvi).rename('UI');\n\n  // IBI (common formulation)\n  var avgVegWater = ndvi.add(mndwi).divide(2);\n  var ibi = safeDivide(ndbi.subtract(avgVegWater), ndbi.add(avgVegWater)).rename('IBI');\n\n  // Brightness: average reflectance (very intuitive for students)\n  var bright = img.select(['blue','green','red','nir','swir1','swir2'])\n    .reduce(ee.Reducer.mean())\n    .rename('BRIGHT');\n\n  return img.addBands([ndvi, mndwi, ndbi, ui, ibi, bright]);\n}\n\n\n\nNow, all the stuff we defined above will start to get used. We need to build a stack of cleaned images per year that cover our region of interest, are masked for clouds etc. For years before 2013, we’ll use Landsat 5 and 7, for years afterwards, we’ll use Landsat 8 and 9. Note that we’re using the variables defined at the top to control things like maximum cloud coverage etc.\n/**************************************************************\n * BUILD A PER-YEAR IMAGE COLLECTION (filter first, then map)\n *\n * Steps:\n *  - Choose a sensor group depending on year (&lt;=2012 uses L5+L7; &gt;=2013 uses L8+L9)\n *  - Filter by AOI and dry-season date window\n *  - Filter by CLOUD_COVER metadata, sort clearest-first, keep top N scenes\n *  - Map preprocessing (mask + scale + rename bands)\n **************************************************************/\n\nfunction getSeasonDateRange(year) {\n  year = ee.Number(year);\n\n  var start = ee.Date.fromYMD(year, seasonStartMonth, 1);\n\n  // endMonth is inclusive-ish: we use the first day of the next month as end\n  var end = ee.Date.fromYMD(year, seasonEndMonth + 1, 1);\n\n  return {start: start, end: end};\n}\n\nfunction collectionForYear(year) {\n  year = ee.Number(year);\n  var range = getSeasonDateRange(year);\n\n  // Choose sensors based on era\n  var rawCollection = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      landsat5_raw.merge(landsat7_raw),   // 2000–2012 era\n      landsat8_raw.merge(landsat9_raw)    // 2013+ era\n    )\n  );\n\n  // Filter on raw metadata BEFORE preprocessing\n  var filtered = rawCollection\n    .filterBounds(aoi)\n    .filterDate(range.start, range.end)\n    .filterMetadata('CLOUD_COVER', 'less_than', maxCloudCoverPercent)\n    .sort('CLOUD_COVER')\n    .limit(maxScenesPerYear);\n\n  // Now map preprocessing and band harmonisation\n  var prepped = ee.ImageCollection(\n    ee.Algorithms.If(\n      year.lte(2012),\n      filtered.map(prepLandsat57),\n      filtered.map(prepLandsat89)\n    )\n  );\n\n  return prepped;\n}\n\n\n\nYou thought we’d never get here! The next bit defines how the composite gets done (using the median), adds the relevant indices and applies it to all the years.\n/**************************************************************\n * ANNUAL COMPOSITE (median) + indices\n *\n * The “annual mosaic” is the median of all (masked) images in the season.\n * Then we add indices so the final image is ready for classification.\n **************************************************************/\n\nfunction annualComposite(year) {\n  year = ee.Number(year);\n\n  var col = collectionForYear(year);\n\n  // Diagnostics: useful for teaching and debugging\n  print('Year', year, 'images used:', col.size());\n\n  // Build the composite and add feature indices\n  var composite = addFeatureIndices(col.median())\n    .clip(aoi)\n    .set('year', year)\n    // time_start used later if we chart time series\n    .set('system:time_start', ee.Date.fromYMD(year, 3, 1).millis());\n\n  return composite;\n}\n\n// Build mosaics for all target years\nvar annualMosaics = ee.ImageCollection(targetYears.map(annualComposite));\nprint('Annual mosaics:', annualMosaics);\n\n\n\n/**************************************************************\n * DISPLAY (choose one year to show)\n *\n * Show:\n *  - RGB (natural colour)\n *  - False colour (NIR / red / green) helps see vegetation strongly\n *  - Indices (NDVI, MNDWI, NDBI, UI, IBI)\n *\n * Note: we use visualize() for the RGB layers because it makes map\n * rendering snappier in the Code Editor.\n **************************************************************/\n\nvar displayYear = 2020;\nvar mosaicToShow = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', displayYear)).first()\n);\n\n// Always sanity-check the bands (helpful if something returns empty)\nprint('Bands in displayed mosaic:', mosaicToShow.bandNames());\n\n// RGB and false colour previews\nMap.addLayer(\n  mosaicToShow.select(['red','green','blue']).visualize(rgbVis),\n  {},\n  'RGB ' + displayYear\n);\n\nMap.addLayer(\n  mosaicToShow.select(['nir','red','green']).visualize(rgbVis),\n  {},\n  'False colour (NIR/R/G) ' + displayYear\n);\n\n// Index layers (simple min/max for teaching; no palettes needed)\nMap.addLayer(mosaicToShow.select('NDVI'),  {min: -0.2, max: 0.8}, 'NDVI');\nMap.addLayer(mosaicToShow.select('MNDWI'), {min: -0.6, max: 0.6}, 'MNDWI');\nMap.addLayer(mosaicToShow.select('NDBI'),  {min: -0.6, max: 0.6}, 'NDBI');\nMap.addLayer(mosaicToShow.select('UI'),    {min: -1.0, max: 1.0}, 'UI (= NDBI - NDVI)');\nMap.addLayer(mosaicToShow.select('IBI'),   {min: -1.0, max: 1.0}, 'IBI');\nMap.addLayer(mosaicToShow.select('BRIGHT'),{min: 0.0,  max: 0.4}, 'Brightness');\n\n\n\nFor repeatibility and further analysis, we’ll export the data to your Google Drive. We’ll use the standard GeoTIFF format, with internal compression to make the data more manageable. This snippet exports a single year, make sure you’re happy with it before exporting all the years.\n/**************************************************************\n * EXPORT ONE YEAR MOSAIC TO GOOGLE DRIVE\n *\n * Pattern:\n *  - Export ONE year first (students learn exports without waiting ages)\n *  - Later, export all years or the classification outputs\n **************************************************************/\n\n// Uncomment to export the displayed year mosaic (multiband)\n \nExport.image.toDrive({\n  image: mosaicToShow, // includes SR bands + indices\n  description: cityName + '_Mosaic_' + displayYear,\n  folder: 'GEE_exports',\n  fileNamePrefix: cityName + '_Mosaic_' + displayYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13,\n  fileFormat: 'GeoTIFF',\n  formatOptions: {\n    cloudOptimized: true,   // Makes it a COG\n    compression: 'DEFLATE' // Powerful lossless compression\n  }\n});\n\n\n\n\n\nOnce you’re happy with the single year export, it’s time to expoert all the years in one go.\n/**************************************************************\n * EXPORT ALL YEARS (one multiband GeoTIFF per year)\n * - Exports each annual mosaic (SR bands + indices) to Google Drive\n * - GeoTIFF options: cloud-optimised (COG) + DEFLATE compression\n **************************************************************/\n\n// Folder in Google Drive\nvar exportFolder = 'GEE_exports';\n\n// Choose a scale appropriate for Landsat SR\nvar exportScale = 30;\n\n// Loop over years and create one export task per year\ntargetYears.forEach(function(year) {\n\n  var img = ee.Image(\n    annualMosaics.filter(ee.Filter.eq('year', year)).first()\n  );\n\n  Export.image.toDrive({\n    image: img,                       // multiband: SR + indices\n    description: cityName + '_Mosaic_' + year,\n    folder: exportFolder,\n    fileNamePrefix: cityName + '_Mosaic_' + year,\n    region: aoi,\n    scale: exportScale,\n    maxPixels: 1e13,\n    fileFormat: 'GeoTIFF',\n    formatOptions: {\n      cloudOptimized: true,\n      compression: 'DEFLATE'\n    }\n  });\n\n});\n\n\n\nSo far, we have used Google Earth Engine to build annual mosaics for our city of interest. These mosaics exist only temporarily inside the script unless we explicitly save them. In addition to exporting images to Google Drive, Earth Engine allows us to export data as Assets, which are stored permanently within our Earth Engine account. Saving the mosaics as Assets is useful because they can be imported directly into other scripts without re-running the mosaicking workflow, making it easier to separate data preparation from later analysis steps such as classification or time-series analysis.\nTo export an annual mosaic as an Asset, we select one of the mosaics from our annualMosaics ImageCollection and use Export.image.toAsset(). When doing this, we must specify:\n\nan asset ID, which defines where the image will be stored in our Earth Engine Assets,\nthe region to export (our AOI),\nthe spatial resolution (30 m for Landsat data).\n\nThe example below exports the mosaic for a single year. Once the export task has completed, the image will appear in the Assets tab of the Earth Engine Code Editor and can be loaded in other scripts using its asset path.\n// Select the mosaic for the year we want to export\nvar exportYear = 2020;\n\nvar mosaicToExport = ee.Image(\n  annualMosaics.filter(ee.Filter.eq('year', exportYear)).first()\n);\n\n// Export the mosaic as a Google Earth Engine Asset\nExport.image.toAsset({\n  image: mosaicToExport,                 // multiband image (SR + indices)\n  description: cityName + '_Mosaic_' + exportYear,\n  assetId: 'users/YOUR_USERNAME/' + cityName + '_Mosaic_' + exportYear,\n  region: aoi,\n  scale: 30,\n  maxPixels: 1e13\n}); // Change YOUR_USERNAME with your own GEE username!\n\n\n\n\n\n\nWarning\n\n\n\nAfter starting the export, the task will appear in the Tasks tab of the Code Editor. You must manually click Run to begin the export. When it has finished, the mosaic can be reused in other Earth Engine scripts by loading it from the Assets panel, allowing you to work directly with the prepared annual composites without repeating the earlier processing steps.",
    "crumbs": [
      "Download Full PDF",
      "3. Create annual Landsat composites"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\n\n\n\n\n\nflowchart LR\n    %% Define the nodes with simplified text\n    A[\"**1. Data Acquisition**&lt;br/&gt;Landsat & Sentinel\"]\n    B[\"**2. Pre-processing**&lt;br/&gt;Filtering & Mosaicking\"]\n    C[\"**3. Classification**&lt;br/&gt;Random Forest ML\"]\n    D[\"**4. Analysis**&lt;br/&gt;Urban Growth Stats\"]\n\n    %% Simple linear connections\n    A --&gt; B --&gt; C --&gt; D\n\n    %% Clean, professional styling\n    style A fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px\n    style B fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px\n    style C fill:#E3F2FD,stroke:#1976D2,stroke-width:2px\n    style D fill:#FFEBEE,stroke:#C62828,stroke-width:2px\n\n    %% Global font settings for the diagram\n    linkStyle default stroke:#666,stroke-width:2px;\n\n\n Overall flow chart \n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth.",
    "crumbs": [
      "Download Full PDF",
      "1. Introduction"
    ]
  },
  {
    "objectID": "01-intro.html#introduction",
    "href": "01-intro.html#introduction",
    "title": "Introduction",
    "section": "",
    "text": "This practical aims to showcase a fairly typical Earth Observation workflow: use the spaceborne acquired data to delineate or map different areas of interest. The actual application is to map the growth of cities (so to map “built-up” or “non-built-up” areas) over time.\n\n\nWhere cities expand (and where they don’t) affects the everyday life of their citizens: water, sanitation, access to education, jobs, exposure to adverse meteorological conditions, … In West Africa, most of this urban growth happens on the fringes, and is largely informal. This means that there is little information to e.g., provide required services once an area has been settled.\nClimate shocks can nudge these patterns. In many parts of West Africa, drought years hit rain-fed agriculture hard. When rural incomes fall, some people move, sometimes temporarily, sometimes permanently, toward larger cities with potentially better labour/livelihood opportunities. That movement can leave a spatial fingerprint: new, fast-growing neighbourhoods at the edge of the city, often on cheap or marginal land. In recent years, civil strife has also produced a large number of displaced people, compounding the drivers of the influx. Limited resources in this region often mean that information is woefully inadequate for policy makers to act on. Fast and informal growth can also increase risk of flooding (as settlements occur in floodplains, wetlands, or block drainage corridors). Dwellers can also become urban heat hotspots, in addition to issues carried out by lack of sanitation and clean drinking water.\nIn the practical you will: (1) build annual Landsat composites, (2) classify land cover into urban, forest, vegetated, bare soil, and water using a Random Forest model, (3) create an urban-area time series, and (4) infer what’s going on. By the end, you’ll have a reproducible workflow and a set of figures that tell a clear story about how a West African city has changed. While you’ll be focussing on a particular city, the potential is there for running this analyses over larger and larger areas, and using this evidence to test for, e.g., quantising the “why” and the “how” of these migrations.\n\n\n\n\n\n\n\n\nflowchart LR\n    %% Define the nodes with simplified text\n    A[\"**1. Data Acquisition**&lt;br/&gt;Landsat & Sentinel\"]\n    B[\"**2. Pre-processing**&lt;br/&gt;Filtering & Mosaicking\"]\n    C[\"**3. Classification**&lt;br/&gt;Random Forest ML\"]\n    D[\"**4. Analysis**&lt;br/&gt;Urban Growth Stats\"]\n\n    %% Simple linear connections\n    A --&gt; B --&gt; C --&gt; D\n\n    %% Clean, professional styling\n    style A fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px\n    style B fill:#FFFDE7,stroke:#FBC02D,stroke-width:2px\n    style C fill:#E3F2FD,stroke:#1976D2,stroke-width:2px\n    style D fill:#FFEBEE,stroke:#C62828,stroke-width:2px\n\n    %% Global font settings for the diagram\n    linkStyle default stroke:#666,stroke-width:2px;\n\n\n Overall flow chart \n\n\n\nIn this activity, we’ll use data from the Landsat archive. The Landsat family of sensors is the longest continuous data record of EO data (since the early 1970s until now). The Landsat archive provides an incredible record of the changes and evolution of the land surface for the last half century. Using this data is not without challenges though: in order to cover such a large time span, different satellites carrying slightly different versions of the imaging sensor have been used, introducing discrepancies.\nLandsat is an optical sensor, and as such, cannot collect data at night, under clouds or under cloud shadows. In areas with persistent cloud cover, the 16-day revisit period for Landsat can result in a depressing meagre number of actual observations! Additionally, for surface applications, the effect of the atmosphere needs to be taking into account. A lot of effort has gone in homogenising and making the actual data as easy and useful to end users, via a process of “calibration and validation”, and producing derived products that employ sophisticated algorithms to map clouds, correct for sensor differences, etc. While these advanced products are invaluable, it is hard to come up with a degree of quality that satisfies all users, so a degree of additional preprocessing is often a crucial step.\nIn terms of urban growth, you will be looking at a time series data (e.g., from 2000 up to 2025, considering every fifth year), and looking to build a classifier that maps the data recorded by the sensor on each pixel to a fixed set of labels (e.g., “urban”, “water”, “vegetation”, etc.). The classifier needs to be defined, and this is done by gathering a so-called training-validation-testing dataset: you will need to select a large set of pixels and label them using the required class labels. Part of this dataset is used to then “train” the classifier, and part of it should be used to assess the quality of the classification and its uncertainty. Once the classifier is trained and validated, you can produce maps of different classes, and use them to quantify the urban growth.",
    "crumbs": [
      "Download Full PDF",
      "1. Introduction"
    ]
  }
]